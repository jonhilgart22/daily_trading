{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization, TimeDistributed, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "from multiprocessing import Process, Pool\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLIEST_DATE = '2012-01-01'\n",
    "N_COMPONENTS=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REad in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_in_data(check_recent_date=True, recent_date_check=datetime.datetime.now().date()):\n",
    "    dict_of_stocks_and_dfs = {}\n",
    "    for file_ in glob.glob('../data/updated_historical_stock_and_etf_data/*.csv'):\n",
    "        stock_name = file_.rsplit(\"/\")[-1].split('_')[0].lower() \n",
    "        print(f\"Reading in {stock_name}\")\n",
    "        df_  = pd.read_csv(f\"{file_}\")\n",
    "        # ensure we have the most recent data\n",
    "        most_recent_date = pd.to_datetime(df_.date.max())\n",
    "        oldest_date = pd.to_datetime(df_.date.min())\n",
    "        \n",
    "        oldest_date_bool = oldest_date < datetime.datetime(1995,1,1).date()\n",
    "        recent_date_bool = most_recent_date == recent_date_check\n",
    "        \n",
    "        if oldest_date_bool and recent_date_bool:\n",
    "            dict_of_stocks_and_dfs[stock_name] = df_.sort_values('date')\n",
    "        elif oldest_date_bool and not check_recent_date:\n",
    "            dict_of_stocks_and_dfs[stock_name] = df_.sort_values('date')            \n",
    "        else:\n",
    "            print(f\"Stock {stock_name} most recent date is {most_recent_date} oldest date is {oldest_date}. Skipping it\")\n",
    "    return dict_of_stocks_and_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in wal\n",
      "Stock wal most recent date is 2020-05-15 00:00:00 oldest date is 2005-06-29 00:00:00. Skipping it\n",
      "Reading in fsbw\n",
      "Stock fsbw most recent date is 2020-05-15 00:00:00 oldest date is 2012-07-10 00:00:00. Skipping it\n",
      "Reading in pfbc\n",
      "Stock pfbc most recent date is 2020-05-15 00:00:00 oldest date is 2011-07-21 00:00:00. Skipping it\n",
      "Reading in hasi\n",
      "Stock hasi most recent date is 2020-05-15 00:00:00 oldest date is 2013-04-18 00:00:00. Skipping it\n",
      "Reading in mtb\n",
      "Reading in rbb\n",
      "Stock rbb most recent date is 2020-05-15 00:00:00 oldest date is 2017-07-26 00:00:00. Skipping it\n",
      "Reading in jpm\n",
      "Reading in umpq\n",
      "Stock umpq most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in vfh\n",
      "Stock vfh most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in iwm\n",
      "Stock iwm most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in cvbf\n",
      "Stock cvbf most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in fitb\n",
      "Reading in irm\n",
      "Stock irm most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in wfc\n",
      "Reading in cwbc\n",
      "Stock cwbc most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in bsrr\n",
      "Stock bsrr most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in key\n",
      "Reading in ewbc\n",
      "Stock ewbc most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in dlr\n",
      "Stock dlr most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in spy\n",
      "Stock spy most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in ubfo\n",
      "Stock ubfo most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in cvcy\n",
      "Stock cvcy most recent date is 2020-05-15 00:00:00 oldest date is 2005-03-02 00:00:00. Skipping it\n",
      "Reading in hope\n",
      "Stock hope most recent date is 2020-05-15 00:00:00 oldest date is 2011-12-05 00:00:00. Skipping it\n",
      "Reading in pacw\n",
      "Stock pacw most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in ntrs\n",
      "Reading in agnc\n",
      "Stock agnc most recent date is 2020-05-15 00:00:00 oldest date is 2008-05-14 00:00:00. Skipping it\n",
      "Reading in cma\n",
      "Reading in qqq\n",
      "Stock qqq most recent date is 2020-05-15 00:00:00 oldest date is 1999-03-10 00:00:00. Skipping it\n",
      "Reading in nrz\n",
      "Stock nrz most recent date is 2020-05-15 00:00:00 oldest date is 2013-05-02 00:00:00. Skipping it\n",
      "Reading in gbci\n",
      "Stock gbci most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in hiw\n",
      "Stock hiw most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in pbj\n",
      "Stock pbj most recent date is 2020-05-15 00:00:00 oldest date is 2005-06-23 00:00:00. Skipping it\n",
      "Reading in vti\n",
      "Stock vti most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in sui\n",
      "Stock sui most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in c\n",
      "Reading in colb\n",
      "Stock colb most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in srg\n",
      "Stock srg most recent date is 2020-05-15 00:00:00 oldest date is 2015-07-06 00:00:00. Skipping it\n",
      "Reading in dre\n",
      "Stock dre most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in bjk\n",
      "Stock bjk most recent date is 2020-05-15 00:00:00 oldest date is 2008-01-24 00:00:00. Skipping it\n",
      "Reading in lamr\n",
      "Stock lamr most recent date is 2020-05-15 00:00:00 oldest date is 1996-08-02 00:00:00. Skipping it\n",
      "Reading in wabc\n",
      "Stock wabc most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in bac\n",
      "Reading in bku\n",
      "Stock bku most recent date is 2020-05-15 00:00:00 oldest date is 2011-02-01 00:00:00. Skipping it\n",
      "Reading in bmrc\n",
      "Stock bmrc most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in xrt\n",
      "Stock xrt most recent date is 2020-05-15 00:00:00 oldest date is 2006-06-22 00:00:00. Skipping it\n",
      "Reading in vno\n",
      "Stock vno most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in usb\n",
      "Reading in caty\n",
      "Stock caty most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in sivb\n",
      "Stock sivb most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in boch\n",
      "Stock boch most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in stt\n",
      "Reading in pej\n",
      "Stock pej most recent date is 2020-05-15 00:00:00 oldest date is 2005-06-23 00:00:00. Skipping it\n",
      "Reading in pnc\n",
      "Reading in are\n",
      "Stock are most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in dia\n",
      "Stock dia most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in cpg\n",
      "Stock cpg most recent date is 2020-05-15 00:00:00 oldest date is 2006-07-21 00:00:00. Skipping it\n",
      "Reading in frc\n",
      "Stock frc most recent date is 2020-05-15 00:00:00 oldest date is 2010-12-13 00:00:00. Skipping it\n",
      "Reading in pei\n",
      "Stock pei most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in cpt\n",
      "Stock cpt most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in reg\n",
      "Stock reg most recent date is 2020-05-15 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n"
     ]
    }
   ],
   "source": [
    "dict_of_stocks_and_dfs = read_in_data(recent_date_check=datetime.datetime(2020,5,15).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(dict_of_stocks_and_dfs.keys())-1) * len(dict_of_stocks_and_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_correlation_dfs(dict_of_stocks_and_dfs, n_day_rolling_features_list=[ 5, 7, 10, 30, 180, 365], verbose=False):\n",
    "    \"\"\"\n",
    "    Create correlation + variance based  upon daily closing stock prices for given date ranges\n",
    "    \n",
    "    also include daily volume\n",
    "    \n",
    "    We are trying to  predict 7 day correaltion\n",
    "    \"\"\"\n",
    "\n",
    "    stock_features_dict = defaultdict(pd.DataFrame)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    start = time.time()\n",
    "    n_stocks = len(dict_of_stocks_and_dfs.keys())\n",
    "    final_feature_df = create_date_dummy_df()\n",
    "    pairs_of_stocks = []\n",
    "    \n",
    "    for idx, first_stock_name in enumerate(dict_of_stocks_and_dfs.keys()):\n",
    "        print('')\n",
    "        print(f\"Finished {idx/n_stocks} pct of stocks\")\n",
    "        print('')\n",
    "        for second_idx, second_stock_name in enumerate(dict_of_stocks_and_dfs.keys()):\n",
    "            stock_pair = f\"{first_stock_name}_{second_stock_name}\"\n",
    "            reverse_pair = f\"{second_stock_name}_{first_stock_name}\"\n",
    "            \n",
    "            if (first_stock_name == second_stock_name) or (stock_pair in pairs_of_stocks)  or (reverse_pair in pairs_of_stocks): # pnr -> ual same as ual -> pnr\n",
    "                continue\n",
    "            else:\n",
    "                pairs_of_stocks.append(stock_pair)\n",
    "            if verbose:\n",
    "                print('-------')\n",
    "                print(f\"{first_stock_name} & {second_stock_name}\")\n",
    "                print('-------')\n",
    "            \n",
    "            # here the date is not the index, yet\n",
    "            first_stock_df = dict_of_stocks_and_dfs[f\"{first_stock_name}\"].loc[ \n",
    "                dict_of_stocks_and_dfs[f\"{first_stock_name}\"].date.isin(dict_of_stocks_and_dfs[f\"{second_stock_name}\"].date), :]\n",
    "\n",
    "            #  filter second df by the dates in first\n",
    "\n",
    "            # here the date is not the index, yet\n",
    "            second_stock_df = dict_of_stocks_and_dfs[f\"{second_stock_name}\"].loc[ \n",
    "                dict_of_stocks_and_dfs[f\"{second_stock_name}\"].date.isin(first_stock_df.date), :]\n",
    "            \n",
    "            # set the date as an index and sort by date\n",
    "            first_stock_df = first_stock_df.sort_values('date')\n",
    "            second_stock_df = second_stock_df.sort_values('date')\n",
    "\n",
    "            first_stock_df = first_stock_df.set_index('date')\n",
    "            second_stock_df = second_stock_df.set_index('date')\n",
    "            \n",
    "            all_features_df = pd.DataFrame()\n",
    "            for rolling_idx, rolling_day in enumerate(n_day_rolling_features_list):\n",
    "                if verbose:\n",
    "                    print(f\"Rolling calculations for {rolling_day}\")\n",
    "                features_df = create_correlation_and_variance_features(\n",
    "                    first_stock_df, second_stock_df, rolling_day, final_feature_df, \n",
    "                    first_stock_name=first_stock_name, second_stock_name=second_stock_name)\n",
    "                   \n",
    "                current_feature_cols = set(features_df.columns)\n",
    "                final_feature_cols = set(final_feature_df.columns)\n",
    "\n",
    "                \n",
    "                if (f\"{first_stock_name}_volume\" not in final_feature_df.columns) and (rolling_idx == 0):\n",
    "                    features_df[f\"{first_stock_name}_volume\"] = list(first_stock_df.volume)\n",
    "                \n",
    "                if (f\"{second_stock_name}_volume\" not in final_feature_df.columns) and (rolling_idx == 0):\n",
    "                    features_df[f\"{second_stock_name}_volume\"] = list(second_stock_df.volume)\n",
    "                    \n",
    "                if rolling_idx == 0: \n",
    "                    all_features_df = features_df\n",
    "                else:\n",
    "                    all_features_df = all_features_df.join(features_df, on='date', lsuffix='_left')\n",
    "            \n",
    "\n",
    "                    \n",
    "            all_features_df.index = pd.to_datetime(all_features_df.index)\n",
    "            final_feature_df = final_feature_df.join(all_features_df, on='date')\n",
    "\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(f\"Building all features took {(end-start)/60} minutes\")\n",
    "                start = time.time()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total time {(end_time-start_time) / 60} minutes for {len(pairs_of_stocks)} pairs\")\n",
    "    final_feature_df = add_time_feature(final_feature_df)\n",
    "    return final_feature_df, pairs_of_stocks\n",
    "            \n",
    "        \n",
    "\n",
    "# Note: will eventuall need to add in 0s for stocks withour correlation data with other stocks due to date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_dummy_df(start_date=datetime.datetime(1980,1,1), n_years=50):\n",
    "    \n",
    "    #  create dummy df with dates to join against\n",
    "    list_of_dates  = []\n",
    "    n_days = 365*n_years\n",
    "    start_date = start_date\n",
    "\n",
    "    for i in range(n_days):\n",
    "        list_of_dates.append(start_date + datetime.timedelta(i))\n",
    "    df_ = pd.DataFrame(list_of_dates, columns=['date'])\n",
    "    \n",
    "    df_.date_ =  pd.to_datetime(df_.date)\n",
    "    return df_.set_index('date')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_feature(final_stock_df):\n",
    "    \n",
    "    days = [i.day for i in final_stock_df.index]\n",
    "    months = [i.month for i in final_stock_df.index]\n",
    "    quarters = [i.quarter for i in final_stock_df.index]\n",
    "    years = [i.year for i in final_stock_df.index]\n",
    "    \n",
    "    us_holidays = holidays.UnitedStates()\n",
    "    \n",
    "    h_ = np.array([i in us_holidays for i in final_stock_df.index]).astype(int)\n",
    "\n",
    "\n",
    "    final_stock_df['day'] = days\n",
    "    final_stock_df['month'] = months\n",
    "    final_stock_df['quarter'] = quarters\n",
    "    final_stock_df['year'] = years\n",
    "#     final_stock_df['is_holiday'] = h_\n",
    "    \n",
    "    return final_stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_and_variance_features(first_stock_df, second_stock_df, n_days_stride, final_stock_df, \n",
    "                                             first_stock_name=None, second_stock_name=None, verbose=False):\n",
    "    \"\"\"\n",
    "    n_days_stride: the  number of rolling days to calculate correlation for\n",
    "    \"\"\"\n",
    "    n_rows = len(first_stock_df)\n",
    "\n",
    "    previous_row = 0\n",
    "\n",
    "    features_per_time_period = defaultdict(list)\n",
    "    if verbose:\n",
    "        print(f\"Creating correlations + variance on close for {n_days_stride} days\")\n",
    "    \n",
    "    rolling_close_df = pd.DataFrame(first_stock_df.close.rolling(\n",
    "        n_days_stride).corr(second_stock_df.close)).rename(\n",
    "        {'close': f\"{first_stock_name}_{second_stock_name}_close_corr_rolling_{n_days_stride}_days\"},axis=1).fillna(method='backfill').round(6)\n",
    "\n",
    "    \n",
    "    # add cols\n",
    "    \n",
    "    current_feature_cols = list(final_stock_df.columns)\n",
    "    \n",
    "\n",
    "    # as we go through different pairs will have multiple var / corr for the first stock\n",
    "    # pnc_bar calcualtes corr for pnr\n",
    "    #pnr_bat calculates corr for pnr\n",
    "    # don't want the same cols\n",
    "    if f\"{first_stock_name}_close_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        \n",
    "        rolling_close_std_first_stock =  first_stock_df.close.rolling(n_days_stride).std().fillna(method='backfill').round(6)\n",
    "        rolling_close_df[f\"{first_stock_name}_close_std_rolling_{n_days_stride}_days\"] = rolling_close_std_first_stock\n",
    "        \n",
    "    if f\"{second_stock_name}_close_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        rolling_close_std_second_stock =  second_stock_df.close.rolling(n_days_stride).std().fillna(method='backfill').round( 6)\n",
    "        rolling_close_df[f\"{second_stock_name}_close_std_rolling_{n_days_stride}_days\"] = rolling_close_std_second_stock\n",
    "        \n",
    "    if f\"{first_stock_name}_volume_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        rolling_volume_std_first_stock =  first_stock_df.volume.rolling(n_days_stride).std().fillna(method='backfill').round(6)\n",
    "        rolling_close_df[f\"{first_stock_name}_volume_std_rolling_{n_days_stride}_days\"] = rolling_volume_std_first_stock\n",
    "        \n",
    "    if f\"{second_stock_name}_volume_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        rolling_volume_std_second_stock =  second_stock_df.volume.rolling(n_days_stride).std().fillna(method='backfill').round(6)\n",
    "        rolling_close_df[f\"{second_stock_name}_volume_std_rolling_{n_days_stride}_days\"] = rolling_volume_std_second_stock\n",
    "    \n",
    "    return rolling_close_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished 0.0 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.08333333333333333 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.16666666666666666 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.25 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.3333333333333333 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.4166666666666667 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.5 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.5833333333333334 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.6666666666666666 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.75 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.8333333333333334 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.9166666666666666 pct of stocks\n",
      "\n",
      "Total time 0.16668004989624025 minutes for 66 pairs\n"
     ]
    }
   ],
   "source": [
    "# 2 minutes fo 210 pairs\n",
    "final_stock_df, pairs_of_stocks = build_correlation_dfs(dict_of_stocks_and_dfs, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of stocks is ~300 NOT 990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_stock_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_stock_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep code for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for LSTM model\n",
    "def split_sequences(sequences, n_steps, y_col='pg_so_close_corr_rolling_7_days', start_idx=0, n_val=50, print_idx=100, input_verbose=1): #2200\n",
    "    \"\"\"\n",
    "    sequences = input_data\n",
    "    n_steps = n_days of data to give at a time\n",
    "    \n",
    "    only works for the currently set y_col\n",
    "    \"\"\"\n",
    "    if y_col not in sequences.columns:\n",
    "        raise ValueError('This y col does not exist in this df')\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    X_val, y_val = list(), list()\n",
    "    \n",
    "    n_sequences = len(sequences)\n",
    "    print('n_sequences', n_sequences)\n",
    "\n",
    "    for i in range(start_idx, n_sequences):\n",
    "        if i == start_idx and input_verbose == 1:\n",
    "            print(f\"Training idx start at {i}\")\n",
    "        if (i % print_idx == 0) and (i != 0) and input_verbose==1:\n",
    "            print(f\"Pct finished = {i/n_sequences}\")\n",
    "            \n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps \n",
    "        total_end_ix = end_ix + n_val\n",
    "        # check if we are beyond the dataset\n",
    "        if (total_end_ix) > n_sequences:\n",
    "            print(f\"Training idx end at {end_ix}\")\n",
    "            print('Total idx checked', total_end_ix)\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = np.array(sequences.loc[:, sequences.columns != f\"{y_col}\"][i:end_ix]), np.array(\n",
    "            sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix-1])\n",
    "\n",
    "                                 \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    val_start_idx = start_idx + n_sequences - (start_idx  + n_val -2)\n",
    "    for i in range(val_start_idx, n_sequences):\n",
    "        if i == val_start_idx and input_verbose==1:\n",
    "            print(f\"Val idx start at {val_start_idx}\")\n",
    "        if (i % print_idx == 0) and i != 0 and input_verbose==1:\n",
    "            print(f\"Pct finished for val sequences = {i/n_sequences}\")\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences) and input_verbose==1:\n",
    "            print(f\"Val idx end at {end_ix}\")\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = np.array(sequences.loc[:, sequences.columns != f\"{y_col}\"][i:end_ix]), np.array(\n",
    "            sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix-1])\n",
    "        \n",
    "        \n",
    "        X_val.append(seq_x)\n",
    "        y_val.append(seq_y)\n",
    "    \n",
    "    \n",
    "\n",
    "    X, y, X_val, y_val = array(X), array(y), array(X_val), array(y_val)\n",
    "    \n",
    "    # errors for standard scaler\n",
    "    X = np.nan_to_num(X.astype(np.float32)) # converting to float 32 throws some infinity errors\n",
    "    X_val = np.nan_to_num(X_val.astype(np.float32)) # converting to float 32 throws some infinity errors  \n",
    "    \n",
    "    \n",
    "    scalers = {}\n",
    "    for i in range(X.shape[1]):\n",
    "        scalers[i] = StandardScaler()\n",
    "        X[:, i, :] = scalers[i].fit_transform(X[:, i, :]) \n",
    "    \n",
    "    pca_scalers = {}\n",
    "    N_COMPONENTS=100\n",
    "\n",
    "    new_X = np.zeros((X.shape[0], X.shape[1], N_COMPONENTS))\n",
    "    for i in range(X.shape[1]):\n",
    "        pca_scalers[i] = PCA(n_components=N_COMPONENTS) # ~80%\n",
    "        new_X[:, i, :] = pca_scalers[i].fit_transform(X[:, i, :]) \n",
    "\n",
    "\n",
    "    for i in range(X_val.shape[1]):\n",
    "        X_val[:, i, :] = scalers[i].transform(X_val[:, i, :]) \n",
    "\n",
    "\n",
    "    new_X_val = np.zeros((X_val.shape[0], X_val.shape[1], N_COMPONENTS))\n",
    "    for i in range(X_val.shape[1]):\n",
    "        new_X_val[:, i, :] = pca_scalers[i].transform(X_val[:, i, :]) \n",
    "        \n",
    "   # need  to do this again as standard scaler may have nans\n",
    "    X = np.nan_to_num(X.astype(np.float32)) # converting to float 32 throws some infinity errors\n",
    "    X_val = np.nan_to_num(X_val.astype(np.float32)) # converting to float 32 throws some infinity errors \n",
    "    print('X val shape', X_val.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    return new_X, y, new_X_val, y_val, scalers, pca_scalers\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y, X_val, y_val, scalers, pca_scalers = split_sequences(\n",
    "#     training_data,\n",
    "#     30, start_idx=0, input_verbose=1,\n",
    "#     n_val=40, y_col=f\"wal_mtb_close_corr_rolling_7_days\"\n",
    "# ) # 30 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(n_steps, n_features, n_units=100, dropout_pct=0.05, n_layers = 1):\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    # define CNN model\n",
    "#     model.add(TimeDistributed(Conv2D(n_units, kernel_and_pool_size))\n",
    "#     model.add(TimeDistributed(MaxPooling2D(pool_size=kernel_and_pool_size))\n",
    "#     model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        \n",
    "    model.add(LSTM(n_units, activation='relu', dropout=dropout_pct, return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    for _ in range(n_layers):\n",
    "        model.add(LSTM(n_units, activation='relu', dropout=dropout_pct, return_sequences=True))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_units, activation='relu', dropout=dropout_pct))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(n_units))\n",
    "    model.add(Dense(int(n_units/2)))\n",
    "    model.add(Dense(1))\n",
    "    #Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    #LR = 0.0001\n",
    "    #clipnorm=1., clipvalue=0.5\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_keras_model(30, 7610)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 30, 100)           3084400   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 3,261,601\n",
      "Trainable params: 3,261,001\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PredictionCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.validation_data[0])\n",
    "        print('prediction: {} at epoch: {}'.format(y_pred, epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on all data\n",
    "# predict for the upcoming week\n",
    "\n",
    "def prediction_for_upcoming_week(final_stock_df,pairs_of_stocks,  job_id=None, print_idx=1, n_day_sequences=14, \n",
    "                                 start_date_training_data='2018-01-01', n_validation_sequences=50, input_batch_size=128, \n",
    "                                 input_verbose=1):\n",
    "    \"\"\"\n",
    "    The main entrypoint for training an LSTM network on stock predictions\n",
    "    \n",
    "    :param final_stock_df: The list of stock pairs with correlations over different time ranges, volume\n",
    "    :para pairs_of_stocks: The list of stock pairs\n",
    "    :param print_idx: The number of iterations to pass before printing out progress\n",
    "    :param n_day_sequences: The number of sequences to pass to the LSTM (i.e. the number of days)\n",
    "    :param start_date_training_data: Filter for data before thie date to train on\n",
    "    :param n_validation_sequences: Number of sequences to validate on. Should be >= 50\n",
    "    :param input_batch_size: size of the batches for training NN\n",
    "    :parm input_verbose: if 1, print out everything otherwise, don't\n",
    "    \"\"\"\n",
    "    print('inside function')\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization, TimeDistributed, Conv2D, Flatten, MaxPooling2D\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "    def build_keras_model(n_steps, n_features, n_units=100, dropout_pct=0.05, n_layers = 1):\n",
    "        model = Sequential()\n",
    "\n",
    "\n",
    "        # define CNN model\n",
    "    #     model.add(TimeDistributed(Conv2D(n_units, kernel_and_pool_size))\n",
    "    #     model.add(TimeDistributed(MaxPooling2D(pool_size=kernel_and_pool_size))\n",
    "    #     model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "        model.add(LSTM(n_units, activation='relu', dropout=dropout_pct, return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "        model.add(BatchNormalization())\n",
    "        for _ in range(n_layers):\n",
    "            model.add(LSTM(n_units, activation='relu', dropout=dropout_pct, return_sequences=True))\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LSTM(n_units, activation='relu', dropout=dropout_pct))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(n_units))\n",
    "        model.add(Dense(int(n_units/2)))\n",
    "        model.add(Dense(1))\n",
    "        #Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        #LR = 0.0001\n",
    "        #clipnorm=1., clipvalue=0.5\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False), loss='mse', metrics=['mse'])\n",
    "        return model\n",
    "\n",
    "\n",
    "    # validation needs to be 40 or index error\n",
    "    final_stock_df = final_stock_df.dropna()\n",
    "    final_stock_df = final_stock_df.sort_values(by='date')\n",
    "    # add this to predictions\n",
    "    stock_to_industry = pd.read_csv('../data/Industries stock list - all.csv')\n",
    "    stock_to_industry.symbol = [i.lower() for i in stock_to_industry.symbol]\n",
    "\n",
    "    final_stock_df = final_stock_df.dropna()\n",
    "    most_recent_date = final_stock_df.index.max()\n",
    "\n",
    "    prediction_end = most_recent_date + datetime.timedelta(7)\n",
    "\n",
    "\n",
    "\n",
    "    test_df = final_stock_df.iloc[-n_day_sequences:, :]\n",
    "\n",
    "    \n",
    "\n",
    "    n_days_corr_predictions = 7\n",
    "\n",
    "\n",
    "    pct_change_corr = []\n",
    "    predicted_corr = []\n",
    "    last_corr_for_prediction_day = []\n",
    "    pred_dates = []\n",
    "    first_stock_industries = []\n",
    "    second_stock_industries = []\n",
    "    \n",
    "    first_model = True\n",
    "\n",
    "    start = time.time()\n",
    "    total_n = len(pairs_of_stocks)\n",
    "    \n",
    "    for idx,stock_pairing in enumerate(pairs_of_stocks):\n",
    "        if idx % print_idx == 0 and input_verbose ==1 :\n",
    "            print('----------')\n",
    "            print(f\"Stock pairing = {stock_pairing}\")\n",
    "            print(f\"Pct finished = {idx/total_n}\")\n",
    "        first_stock_name, second_stock_name = stock_pairing.split('_')\n",
    "\n",
    "        first_stock_industries.append(stock_to_industry[stock_to_industry.symbol == first_stock_name].industry.values[0])\n",
    "        second_stock_industries.append(stock_to_industry[stock_to_industry.symbol == second_stock_name].industry.values[0])\n",
    "\n",
    "\n",
    "\n",
    "        pred_col_name = f\"{stock_pairing}_close_corr_rolling_{n_days_corr_predictions}_days\"\n",
    "\n",
    "        # remove the current 7-day corr for this stock\n",
    "        # for 7 take rolling 7 days corr to the present day to predict off of\n",
    "        \n",
    "        ## TRAINING AND TESTING DATA\n",
    "        X,y, X_val, y_val, scalers, pca_scalers = split_sequences(\n",
    "            final_stock_df[final_stock_df.index >= f\"{start_date_training_data}\"],\n",
    "            n_day_sequences, start_idx=0, input_verbose=input_verbose,\n",
    "            n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
    "        ) # 30 steps\n",
    "        print('finished splitting sequences')\n",
    "        \n",
    "        train_X, train_y = final_stock_df.loc[:, final_stock_df.columns != f\"{pred_col_name}\"],  final_stock_df[f\"{pred_col_name}\"].shift(-7).fillna(method='ffill') \n",
    "                                                           # get corr from 7 days in the future\n",
    "        test_X, test_y = np.array(test_df.loc[:, test_df.columns != f\"{pred_col_name}\"]),  test_df[f\"{pred_col_name}\"]\n",
    "        test_X = test_X.reshape(1, test_X.shape[0], test_X.shape[1])\n",
    "        test_X = np.nan_to_num(test_X.astype(np.float32))\n",
    "        print('finihsedd text_x')\n",
    "        \n",
    "        for i in range(test_X.shape[1]):\n",
    "            test_X[:, i, :] = scalers[i].transform(test_X[:, i, :]) \n",
    "        test_X = np.nan_to_num(test_X.astype(np.float32))\n",
    "        \n",
    "        new_X_test = np.zeros((test_X.shape[0], test_X.shape[1], N_COMPONENTS))\n",
    "        for i in range(test_X.shape[1]):\n",
    "            new_X_test[:, i, :] = pca_scalers[i].transform(test_X[:, i, :]) \n",
    "        print('finished test x pca')\n",
    "        \n",
    "\n",
    "        \n",
    "#         return X,y, X_val, y_val, new_X_test\n",
    "        ## END TRAINING AND TESTING DATA \n",
    "        \n",
    "        \n",
    "        if first_model:\n",
    "            print('building model')\n",
    "            smaller_model = build_keras_model(X.shape[1],X.shape[2])\n",
    "            print(smaller_model.summary())\n",
    "            \n",
    "        # test again at 700 epochs\n",
    "        if first_model:\n",
    "            start = time.time()\n",
    "            # 800 epochs\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=200, verbose=1, restore_best_weights=True)\n",
    "            print('starting training')\n",
    "            history = smaller_model.fit(x=X, y=y, batch_size=input_batch_size, epochs=200, verbose=input_verbose, \n",
    "                      validation_data=(X_val, y_val), shuffle=False,  use_multiprocessing=False, callbacks=[early_stopping])\n",
    "            end=time.time()\n",
    "\n",
    "            print((end-start)/60,' minutes')\n",
    "        else:\n",
    "\n",
    "            # Freeze the layers except the last 5 layers\n",
    "            for layer in smaller_model.layers[:-3]:\n",
    "                layer.trainable = False\n",
    "            # Check the trainable status of the individual layers\n",
    "\n",
    "#             for layer in smaller_model.layers:\n",
    "#                 print(layer, layer.trainable)\n",
    "\n",
    "#             smaller_model.compile(optimizer=Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False), loss='mse', metrics=['mse'])\n",
    "            \n",
    "            start = time.time()\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=1, restore_best_weights=True)\n",
    "            smaller_model = build_keras_model(X.shape[1],X.shape[2])\n",
    "            print(smaller_model.summary())\n",
    "            history = smaller_model.fit(x=X, y=y, batch_size=input_batch_size, epochs=200, verbose=input_verbose, \n",
    "                      validation_data=(X_val, y_val), shuffle=False,  use_multiprocessing=False, callbacks=[early_stopping])\n",
    "            end=time.time()\n",
    "            print((end-start)/60,' minutes')\n",
    "\n",
    "    \n",
    "        history_df  = pd.DataFrame(history.history)\n",
    "        history_df[['mse', 'val_mse']].iloc[-100:, :].plot()\n",
    "        plt.show()\n",
    "        prediction = smaller_model.predict(new_X_test)[0][0] \n",
    "\n",
    "        if idx % print_idx==0:\n",
    "            print(f\"Prediction = {prediction}\")\n",
    "\n",
    "\n",
    "\n",
    "        last_corr_date = train_y.index.max()\n",
    "        last_corr = train_y[train_y.index.max()]  \n",
    "        if idx % print_idx==0:\n",
    "            print(f\"Last corr = {last_corr}\")\n",
    "\n",
    "        pred_dates.append(most_recent_date)\n",
    "        predicted_corr.append(prediction)\n",
    "        last_corr_for_prediction_day.append(last_corr)\n",
    "        \n",
    "        if input_verbose==1 and idx % print_idx==0:\n",
    "            print(f\"{stock_pairing} corr7-day corr of close from {most_recent_date} to {prediction_end} is {prediction} \")\n",
    "        \n",
    "        first_model = False\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Predictions took {(end-start)/60} mins\")\n",
    "\n",
    "    squarred_difference = (np.array(last_corr_for_prediction_day)-np.array(predicted_corr))**2\n",
    "\n",
    "    prediction_df = pd.DataFrame({ 'pred_date_start':pred_dates,'stock_pair':pairs_of_stocks,   'first_stock_industry': first_stock_industries, \n",
    "                   'second_stock_industry': second_stock_industries,\n",
    "                   'predicted_corr': predicted_corr, 'last_7_day_corr_for_pred_date_start': last_corr_for_prediction_day, \n",
    "            'squarred_diff_7_day_cor': (np.array(last_corr_for_prediction_day)-np.array(predicted_corr))**2\n",
    "                 })\n",
    "    \n",
    "    if job_id:\n",
    "        tmp_filepath = '../data/lstm_tmp_prediction_dfs'\n",
    "        if not os.path.isdir(f\"{tmp_filepath}\"):\n",
    "            os.mkdir(f\"{tmp_filepath}\")\n",
    "        prediction_df.to_csv(\n",
    "        f'{tmp_filepath}/{job_id}_lstm_test_predictions_{most_recent_date}-{prediction_end}.csv', index=False)\n",
    "    else:\n",
    "        prediction_df.to_csv(\n",
    "    f'../data/predictions/lstm_test_predictions_{most_recent_date}-{prediction_end}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs_of_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]\n",
    "\n",
    "def do_job(job_id, stock_pairs, data_slice):\n",
    "    all_prediction_df = []\n",
    "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
    "\n",
    "\n",
    "\n",
    "def dispatch_jobs(data, job_number, pairs):\n",
    "    total = len(pairs)\n",
    "    chunk_size = total / job_number\n",
    "    slice = chunks(pairs, int(chunk_size))\n",
    "    jobs = []\n",
    "\n",
    "    for i, pair in enumerate(slice):\n",
    "        j = Process(target=do_job, args=(i, pair, data))\n",
    "        jobs.append(j)\n",
    "    for j in jobs:\n",
    "        j.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside function\n",
      "inside function\n",
      "inside function\n",
      "inside function\n",
      "inside function\n",
      "inside function\n",
      "inside function\n",
      "inside function\n",
      "inside function\n",
      "----------\n",
      "Stock pairing = mtb_jpm\n",
      "Pct finished = 0.0\n",
      "----------\n",
      "Stock pairing = mtb_usb\n",
      "n_sequences 597\n",
      "Pct finished = 0.0\n",
      "Training idx start at 0\n",
      "n_sequences 597\n",
      "Training idx start at 0\n",
      "----------\n",
      "Stock pairing = jpm_c\n",
      "Pct finished = 0.0\n",
      "n_sequences 597\n",
      "Training idx start at 0\n",
      "----------\n",
      "Stock pairing = fitb_cma\n",
      "Pct finished = 0.0\n",
      "n_sequences 597\n",
      "Training idx start at 0\n",
      "----------\n",
      "----------\n",
      "Stock pairing = key_c\n",
      "Stock pairing = wfc_cma\n",
      "Pct finished = 0.0\n",
      "Pct finished = 0.0\n",
      "n_sequences 597\n",
      "n_sequences 597\n",
      "Training idx start at 0\n",
      "Training idx start at 0\n",
      "----------\n",
      "Stock pairing = ntrs_usb\n",
      "Pct finished = 0.0\n",
      "n_sequences 597\n",
      "Training idx start at 0\n",
      "----------\n",
      "Stock pairing = usb_pnc\n",
      "Pct finished = 0.0\n",
      "n_sequences 597\n",
      "Training idx start at 0\n",
      "----------\n",
      "Stock pairing = c_bac\n",
      "Pct finished = 0.0\n",
      "n_sequences 597\n",
      "Training idx start at 0\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.5025125628140703\n",
      "Pct finished = 0.5025125628140703\n",
      "Pct finished = 0.5025125628140703\n",
      "Pct finished = 0.5025125628140703\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     mp.set_start_method('spawn', force=True)\n",
    "    final_stock_df = final_stock_df.dropna()\n",
    "    num_workers = mp.cpu_count()  \n",
    "    dispatch_jobs(final_stock_df, num_workers , pairs_of_stocks[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stock_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_model = build_keras_model(X.shape[1],X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_273 (LSTM)              (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_264 (Bat (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_274 (LSTM)              (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_265 (Bat (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_275 (LSTM)              (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_266 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_270 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_271 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_272 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 257,601\n",
      "Trainable params: 257,001\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "smaller_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Stock pairing = wal_mtb\n",
      "Pct finished = 0.0\n",
      "n_sequences 597\n",
      "Training idx start at 0\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.5025125628140703\n",
      "Pct finished = 0.6700167504187605\n",
      "Pct finished = 0.8375209380234506\n",
      "Training idx end at 548\n",
      "Total idx checked 598\n",
      "Val idx start at 549\n",
      "Val idx end at 598\n",
      "X val shape (35, 14, 4956)\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_270 (LSTM)              (None, 14, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_261 (Bat (None, 14, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_271 (LSTM)              (None, 14, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_262 (Bat (None, 14, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_272 (LSTM)              (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_263 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_267 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_268 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_269 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 257,601\n",
      "Trainable params: 257,001\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "starting training\n",
      "Train on 534 samples, validate on 35 samples\n",
      "Epoch 1/200\n",
      "534/534 [==============================] - 5s 9ms/sample - loss: 2.9559 - mse: 2.9559 - val_loss: 7.3388 - val_mse: 7.3388\n",
      "Epoch 2/200\n",
      "534/534 [==============================] - 0s 664us/sample - loss: 2.1165 - mse: 2.1165 - val_loss: 2.4950 - val_mse: 2.4950\n",
      "Epoch 3/200\n",
      "534/534 [==============================] - 0s 699us/sample - loss: 1.7388 - mse: 1.7388 - val_loss: 1.4981 - val_mse: 1.4981\n",
      "Epoch 4/200\n",
      "534/534 [==============================] - 0s 705us/sample - loss: 1.5410 - mse: 1.5410 - val_loss: 1.1165 - val_mse: 1.1165\n",
      "Epoch 5/200\n",
      "534/534 [==============================] - 0s 701us/sample - loss: 1.3267 - mse: 1.3267 - val_loss: 1.0767 - val_mse: 1.0767\n",
      "Epoch 6/200\n",
      "534/534 [==============================] - 0s 688us/sample - loss: 1.1348 - mse: 1.1348 - val_loss: 1.1074 - val_mse: 1.1074\n",
      "Epoch 7/200\n",
      "512/534 [===========================>..] - ETA: 0s - loss: 1.1067 - mse: 1.1067WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534/534 [==============================] - 0s 639us/sample - loss: 1.1145 - mse: 1.1145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1183-f8f8bf35305a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 300 pairs take 10 hours sequentially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# X,y, X_val, y_val, scalers, pca_scalers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprediction_for_upcoming_week\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_stock_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_of_stocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1182-2ba139ba7626>\u001b[0m in \u001b[0;36mprediction_for_upcoming_week\u001b[0;34m(final_stock_df, pairs_of_stocks, job_id, print_idx, n_day_sequences, start_date_training_data, n_validation_sequences, input_batch_size, input_verbose)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             history = smaller_model.fit(x=X, y=y, batch_size=input_batch_size, epochs=200, verbose=input_verbose, \n\u001b[0;32m--> 107\u001b[0;31m                       validation_data=(X_val, y_val), shuffle=False,  use_multiprocessing=False, callbacks=[early_stopping])\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                       total_epochs=1)\n\u001b[0m\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m    397\u001b[0m                                  prefix='val_')\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 6.5 minutes for 10 stocks\n",
    "final_stock_df = final_stock_df.dropna()\n",
    "# test 14 day period instead of 30\n",
    "# no dice\n",
    "# test smaller network smae LR\n",
    "# no dice\n",
    "# test since 2016 data\n",
    "# no dice\n",
    "# test smaller learning rate \n",
    "# no dice\n",
    "# test smaller batch size\n",
    "# nothing\n",
    "# test batch size  \n",
    "\n",
    "\n",
    "# so the solution was less validation data\n",
    "\n",
    "# wal_cwbc is turns out needed less training data\n",
    "\n",
    "# test a new model for each pair, 200 epochs per . 2 minutes per 200 epochs\n",
    "# 300 pairs take 10 hours sequentially\n",
    "# X,y, X_val, y_val, scalers, pca_scalers \n",
    "training_data= prediction_for_upcoming_week(final_stock_df, pairs_of_stocks[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wal_mtb_close_corr_rolling_5_days</th>\n",
       "      <th>wal_close_std_rolling_5_days</th>\n",
       "      <th>mtb_close_std_rolling_5_days</th>\n",
       "      <th>wal_volume_std_rolling_5_days</th>\n",
       "      <th>mtb_volume_std_rolling_5_days</th>\n",
       "      <th>wal_volume</th>\n",
       "      <th>mtb_volume</th>\n",
       "      <th>wal_mtb_close_corr_rolling_7_days</th>\n",
       "      <th>wal_close_std_rolling_7_days</th>\n",
       "      <th>mtb_close_std_rolling_7_days</th>\n",
       "      <th>...</th>\n",
       "      <th>cpt_reg_close_corr_rolling_5_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_7_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_10_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_30_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_180_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_365_days</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>0.969491</td>\n",
       "      <td>0.242590</td>\n",
       "      <td>0.540629</td>\n",
       "      <td>71459.939653</td>\n",
       "      <td>82750.470242</td>\n",
       "      <td>389396.0</td>\n",
       "      <td>424392.0</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.644249</td>\n",
       "      <td>0.961794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522325</td>\n",
       "      <td>0.887355</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.098938</td>\n",
       "      <td>0.464441</td>\n",
       "      <td>-0.388416</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>0.666366</td>\n",
       "      <td>0.223271</td>\n",
       "      <td>0.870747</td>\n",
       "      <td>76557.999255</td>\n",
       "      <td>87630.342220</td>\n",
       "      <td>338118.0</td>\n",
       "      <td>437559.0</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.438623</td>\n",
       "      <td>0.848135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091648</td>\n",
       "      <td>0.765359</td>\n",
       "      <td>0.908782</td>\n",
       "      <td>0.126810</td>\n",
       "      <td>0.486648</td>\n",
       "      <td>-0.386528</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>0.721908</td>\n",
       "      <td>0.240520</td>\n",
       "      <td>1.102034</td>\n",
       "      <td>158775.994394</td>\n",
       "      <td>143127.496064</td>\n",
       "      <td>611753.0</td>\n",
       "      <td>628314.0</td>\n",
       "      <td>0.662799</td>\n",
       "      <td>0.207961</td>\n",
       "      <td>0.931213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962028</td>\n",
       "      <td>0.961260</td>\n",
       "      <td>0.953959</td>\n",
       "      <td>0.322918</td>\n",
       "      <td>0.508217</td>\n",
       "      <td>-0.386158</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.578692</td>\n",
       "      <td>0.463271</td>\n",
       "      <td>1.137299</td>\n",
       "      <td>128230.249714</td>\n",
       "      <td>94217.107971</td>\n",
       "      <td>425203.0</td>\n",
       "      <td>549691.0</td>\n",
       "      <td>0.590420</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>0.969475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975050</td>\n",
       "      <td>0.977253</td>\n",
       "      <td>0.965353</td>\n",
       "      <td>0.471573</td>\n",
       "      <td>0.530037</td>\n",
       "      <td>-0.384616</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.280348</td>\n",
       "      <td>0.400849</td>\n",
       "      <td>0.870994</td>\n",
       "      <td>111338.714266</td>\n",
       "      <td>84419.407121</td>\n",
       "      <td>348035.0</td>\n",
       "      <td>488878.0</td>\n",
       "      <td>0.551488</td>\n",
       "      <td>0.407472</td>\n",
       "      <td>0.929618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981058</td>\n",
       "      <td>0.978178</td>\n",
       "      <td>0.967308</td>\n",
       "      <td>0.543734</td>\n",
       "      <td>0.548609</td>\n",
       "      <td>-0.381319</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>0.766228</td>\n",
       "      <td>0.868637</td>\n",
       "      <td>2.179638</td>\n",
       "      <td>52817.185241</td>\n",
       "      <td>149191.991991</td>\n",
       "      <td>757758.0</td>\n",
       "      <td>897767.0</td>\n",
       "      <td>0.748908</td>\n",
       "      <td>0.777775</td>\n",
       "      <td>2.790674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820796</td>\n",
       "      <td>0.621151</td>\n",
       "      <td>0.773188</td>\n",
       "      <td>0.790651</td>\n",
       "      <td>0.947226</td>\n",
       "      <td>0.762557</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-12</th>\n",
       "      <td>0.959172</td>\n",
       "      <td>1.505380</td>\n",
       "      <td>3.481016</td>\n",
       "      <td>93169.966213</td>\n",
       "      <td>232839.250455</td>\n",
       "      <td>842800.0</td>\n",
       "      <td>1054813.0</td>\n",
       "      <td>0.921566</td>\n",
       "      <td>1.291135</td>\n",
       "      <td>3.298373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>0.833468</td>\n",
       "      <td>0.809434</td>\n",
       "      <td>0.793117</td>\n",
       "      <td>0.947982</td>\n",
       "      <td>0.765509</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-13</th>\n",
       "      <td>0.982959</td>\n",
       "      <td>2.297035</td>\n",
       "      <td>5.954967</td>\n",
       "      <td>252559.966011</td>\n",
       "      <td>235833.161623</td>\n",
       "      <td>1243691.0</td>\n",
       "      <td>970433.0</td>\n",
       "      <td>0.965713</td>\n",
       "      <td>1.897660</td>\n",
       "      <td>5.166668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891453</td>\n",
       "      <td>0.860975</td>\n",
       "      <td>0.796259</td>\n",
       "      <td>0.795974</td>\n",
       "      <td>0.948399</td>\n",
       "      <td>0.768252</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14</th>\n",
       "      <td>0.986109</td>\n",
       "      <td>2.456406</td>\n",
       "      <td>6.032050</td>\n",
       "      <td>240029.632601</td>\n",
       "      <td>243475.803784</td>\n",
       "      <td>1044445.0</td>\n",
       "      <td>1155514.0</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>2.123624</td>\n",
       "      <td>5.428586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954624</td>\n",
       "      <td>0.906905</td>\n",
       "      <td>0.810136</td>\n",
       "      <td>0.799692</td>\n",
       "      <td>0.948965</td>\n",
       "      <td>0.771334</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-15</th>\n",
       "      <td>0.932608</td>\n",
       "      <td>1.654282</td>\n",
       "      <td>4.280231</td>\n",
       "      <td>225816.719524</td>\n",
       "      <td>119314.840351</td>\n",
       "      <td>693076.0</td>\n",
       "      <td>861633.0</td>\n",
       "      <td>0.974008</td>\n",
       "      <td>2.243275</td>\n",
       "      <td>6.121549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911496</td>\n",
       "      <td>0.927717</td>\n",
       "      <td>0.892990</td>\n",
       "      <td>0.791171</td>\n",
       "      <td>0.949431</td>\n",
       "      <td>0.774133</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>597 rows × 4957 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            wal_mtb_close_corr_rolling_5_days  wal_close_std_rolling_5_days  \\\n",
       "date                                                                          \n",
       "2018-01-02                           0.969491                      0.242590   \n",
       "2018-01-03                           0.666366                      0.223271   \n",
       "2018-01-04                           0.721908                      0.240520   \n",
       "2018-01-05                           0.578692                      0.463271   \n",
       "2018-01-08                           0.280348                      0.400849   \n",
       "...                                       ...                           ...   \n",
       "2020-05-11                           0.766228                      0.868637   \n",
       "2020-05-12                           0.959172                      1.505380   \n",
       "2020-05-13                           0.982959                      2.297035   \n",
       "2020-05-14                           0.986109                      2.456406   \n",
       "2020-05-15                           0.932608                      1.654282   \n",
       "\n",
       "            mtb_close_std_rolling_5_days  wal_volume_std_rolling_5_days  \\\n",
       "date                                                                      \n",
       "2018-01-02                      0.540629                   71459.939653   \n",
       "2018-01-03                      0.870747                   76557.999255   \n",
       "2018-01-04                      1.102034                  158775.994394   \n",
       "2018-01-05                      1.137299                  128230.249714   \n",
       "2018-01-08                      0.870994                  111338.714266   \n",
       "...                                  ...                            ...   \n",
       "2020-05-11                      2.179638                   52817.185241   \n",
       "2020-05-12                      3.481016                   93169.966213   \n",
       "2020-05-13                      5.954967                  252559.966011   \n",
       "2020-05-14                      6.032050                  240029.632601   \n",
       "2020-05-15                      4.280231                  225816.719524   \n",
       "\n",
       "            mtb_volume_std_rolling_5_days  wal_volume  mtb_volume  \\\n",
       "date                                                                \n",
       "2018-01-02                   82750.470242    389396.0    424392.0   \n",
       "2018-01-03                   87630.342220    338118.0    437559.0   \n",
       "2018-01-04                  143127.496064    611753.0    628314.0   \n",
       "2018-01-05                   94217.107971    425203.0    549691.0   \n",
       "2018-01-08                   84419.407121    348035.0    488878.0   \n",
       "...                                   ...         ...         ...   \n",
       "2020-05-11                  149191.991991    757758.0    897767.0   \n",
       "2020-05-12                  232839.250455    842800.0   1054813.0   \n",
       "2020-05-13                  235833.161623   1243691.0    970433.0   \n",
       "2020-05-14                  243475.803784   1044445.0   1155514.0   \n",
       "2020-05-15                  119314.840351    693076.0    861633.0   \n",
       "\n",
       "            wal_mtb_close_corr_rolling_7_days  wal_close_std_rolling_7_days  \\\n",
       "date                                                                          \n",
       "2018-01-02                           0.982199                      0.644249   \n",
       "2018-01-03                           0.726948                      0.438623   \n",
       "2018-01-04                           0.662799                      0.207961   \n",
       "2018-01-05                           0.590420                      0.388716   \n",
       "2018-01-08                           0.551488                      0.407472   \n",
       "...                                       ...                           ...   \n",
       "2020-05-11                           0.748908                      0.777775   \n",
       "2020-05-12                           0.921566                      1.291135   \n",
       "2020-05-13                           0.965713                      1.897660   \n",
       "2020-05-14                           0.982818                      2.123624   \n",
       "2020-05-15                           0.974008                      2.243275   \n",
       "\n",
       "            mtb_close_std_rolling_7_days  ...  \\\n",
       "date                                      ...   \n",
       "2018-01-02                      0.961794  ...   \n",
       "2018-01-03                      0.848135  ...   \n",
       "2018-01-04                      0.931213  ...   \n",
       "2018-01-05                      0.969475  ...   \n",
       "2018-01-08                      0.929618  ...   \n",
       "...                                  ...  ...   \n",
       "2020-05-11                      2.790674  ...   \n",
       "2020-05-12                      3.298373  ...   \n",
       "2020-05-13                      5.166668  ...   \n",
       "2020-05-14                      5.428586  ...   \n",
       "2020-05-15                      6.121549  ...   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_5_days  \\\n",
       "date                                            \n",
       "2018-01-02                           0.522325   \n",
       "2018-01-03                          -0.091648   \n",
       "2018-01-04                           0.962028   \n",
       "2018-01-05                           0.975050   \n",
       "2018-01-08                           0.981058   \n",
       "...                                       ...   \n",
       "2020-05-11                           0.820796   \n",
       "2020-05-12                           0.881267   \n",
       "2020-05-13                           0.891453   \n",
       "2020-05-14                           0.954624   \n",
       "2020-05-15                           0.911496   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_7_days  \\\n",
       "date                                            \n",
       "2018-01-02                           0.887355   \n",
       "2018-01-03                           0.765359   \n",
       "2018-01-04                           0.961260   \n",
       "2018-01-05                           0.977253   \n",
       "2018-01-08                           0.978178   \n",
       "...                                       ...   \n",
       "2020-05-11                           0.621151   \n",
       "2020-05-12                           0.833468   \n",
       "2020-05-13                           0.860975   \n",
       "2020-05-14                           0.906905   \n",
       "2020-05-15                           0.927717   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_10_days  \\\n",
       "date                                             \n",
       "2018-01-02                            0.928270   \n",
       "2018-01-03                            0.908782   \n",
       "2018-01-04                            0.953959   \n",
       "2018-01-05                            0.965353   \n",
       "2018-01-08                            0.967308   \n",
       "...                                        ...   \n",
       "2020-05-11                            0.773188   \n",
       "2020-05-12                            0.809434   \n",
       "2020-05-13                            0.796259   \n",
       "2020-05-14                            0.810136   \n",
       "2020-05-15                            0.892990   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_30_days  \\\n",
       "date                                             \n",
       "2018-01-02                            0.098938   \n",
       "2018-01-03                            0.126810   \n",
       "2018-01-04                            0.322918   \n",
       "2018-01-05                            0.471573   \n",
       "2018-01-08                            0.543734   \n",
       "...                                        ...   \n",
       "2020-05-11                            0.790651   \n",
       "2020-05-12                            0.793117   \n",
       "2020-05-13                            0.795974   \n",
       "2020-05-14                            0.799692   \n",
       "2020-05-15                            0.791171   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_180_days  \\\n",
       "date                                              \n",
       "2018-01-02                             0.464441   \n",
       "2018-01-03                             0.486648   \n",
       "2018-01-04                             0.508217   \n",
       "2018-01-05                             0.530037   \n",
       "2018-01-08                             0.548609   \n",
       "...                                         ...   \n",
       "2020-05-11                             0.947226   \n",
       "2020-05-12                             0.947982   \n",
       "2020-05-13                             0.948399   \n",
       "2020-05-14                             0.948965   \n",
       "2020-05-15                             0.949431   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_365_days  day  month  quarter  year  \n",
       "date                                                                        \n",
       "2018-01-02                            -0.388416    2      1        1  2018  \n",
       "2018-01-03                            -0.386528    3      1        1  2018  \n",
       "2018-01-04                            -0.386158    4      1        1  2018  \n",
       "2018-01-05                            -0.384616    5      1        1  2018  \n",
       "2018-01-08                            -0.381319    8      1        1  2018  \n",
       "...                                         ...  ...    ...      ...   ...  \n",
       "2020-05-11                             0.762557   11      5        2  2020  \n",
       "2020-05-12                             0.765509   12      5        2  2020  \n",
       "2020-05-13                             0.768252   13      5        2  2020  \n",
       "2020-05-14                             0.771334   14      5        2  2020  \n",
       "2020-05-15                             0.774133   15      5        2  2020  \n",
       "\n",
       "[597 rows x 4957 columns]"
      ]
     },
     "execution_count": 1135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(X_val).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(new_X_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('daily-trading-nJ43NNNI-py3.7': venv)",
   "language": "python",
   "name": "python37064bitdailytradingnj43nnnipy37venvd9db9359ec934f90b15f4732101b653e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
