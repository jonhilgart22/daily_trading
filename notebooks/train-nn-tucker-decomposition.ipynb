{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorly as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# from nn_train import do_job\n",
    "import random\n",
    "\n",
    "# cna not import into global namespace  for multiprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization, TimeDistributed, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "from multiprocessing import Process, Pool\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import tensorly.decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  REad in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_in_data(check_recent_date=True, recent_date_check=datetime.datetime.now().date()):\n",
    "    dict_of_stocks_and_dfs = {}\n",
    "    for file_ in glob.glob('../data/updated_historical_stock_and_etf_data/*.csv'):\n",
    "        stock_name = file_.rsplit(\"/\")[-1].split('_')[0].lower() \n",
    "        print(f\"Reading in {stock_name}\")\n",
    "        df_  = pd.read_csv(f\"{file_}\")\n",
    "        # ensure we have the most recent data\n",
    "        most_recent_date = pd.to_datetime(df_.date.max())\n",
    "        oldest_date = pd.to_datetime(df_.date.min())\n",
    "        \n",
    "        oldest_date_bool = oldest_date < datetime.datetime(1990,1,2).date()\n",
    "        recent_date_bool = most_recent_date == recent_date_check\n",
    "        \n",
    "        if oldest_date_bool and recent_date_bool:\n",
    "            dict_of_stocks_and_dfs[stock_name] = df_.sort_values('date')\n",
    "        elif oldest_date_bool and not check_recent_date:\n",
    "            dict_of_stocks_and_dfs[stock_name] = df_.sort_values('date')            \n",
    "        else:\n",
    "            print(f\"Stock {stock_name} most recent date is {most_recent_date} oldest date is {oldest_date}. Skipping it\")\n",
    "    return dict_of_stocks_and_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in pnr\n",
      "Stock pnr most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in mcri\n",
      "Stock mcri most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in amrb\n",
      "Stock amrb most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in inn\n",
      "Stock inn most recent date is 2020-06-12 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in ftc\n",
      "Stock ftc most recent date is 2020-06-05 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in six\n",
      "Stock six most recent date is 2020-06-12 00:00:00 oldest date is 2010-05-11 00:00:00. Skipping it\n",
      "Reading in penn\n",
      "Stock penn most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in fitb\n",
      "Stock fitb most recent date is 2020-06-05 00:00:00 oldest date is 1990-03-26 00:00:00. Skipping it\n",
      "Reading in boh\n",
      "Stock boh most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in bjk\n",
      "Stock bjk most recent date is 2020-06-12 00:00:00 oldest date is 2008-01-24 00:00:00. Skipping it\n",
      "Reading in hope\n",
      "Stock hope most recent date is 2020-06-05 00:00:00 oldest date is 2011-12-05 00:00:00. Skipping it\n",
      "Reading in ni\n",
      "Reading in well\n",
      "Stock well most recent date is 2020-06-05 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in bac\n",
      "Reading in seas\n",
      "Stock seas most recent date is 2020-06-12 00:00:00 oldest date is 2013-04-19 00:00:00. Skipping it\n",
      "Reading in wfc\n",
      "Stock wfc most recent date is 2020-06-05 00:00:00 oldest date is 1984-11-01 00:00:00. Skipping it\n",
      "Reading in gden\n",
      "Stock gden most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in htbk\n",
      "Stock htbk most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in nrz\n",
      "Stock nrz most recent date is 2020-06-05 00:00:00 oldest date is 2013-05-02 00:00:00. Skipping it\n",
      "Reading in inn\n",
      "Stock inn most recent date is 2020-06-05 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in efx\n",
      "Reading in ovly\n",
      "Stock ovly most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in xrt\n",
      "Stock xrt most recent date is 2020-06-12 00:00:00 oldest date is 2006-06-22 00:00:00. Skipping it\n",
      "Reading in ual\n",
      "Stock ual most recent date is 2020-06-12 00:00:00 oldest date is 2006-01-25 00:00:00. Skipping it\n",
      "Reading in nly\n",
      "Stock nly most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in bku\n",
      "Stock bku most recent date is 2020-06-05 00:00:00 oldest date is 2011-02-01 00:00:00. Skipping it\n",
      "Reading in ph\n",
      "Reading in ccb\n",
      "Stock ccb most recent date is 2020-06-05 00:00:00 oldest date is 2018-07-18 00:00:00. Skipping it\n",
      "Reading in bcml\n",
      "Stock bcml most recent date is 2020-06-05 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in ftc\n",
      "Stock ftc most recent date is 2020-06-05 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in cvcy\n",
      "Stock cvcy most recent date is 2020-06-05 00:00:00 oldest date is 2005-03-02 00:00:00. Skipping it\n",
      "Reading in frt\n",
      "Stock frt most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in tcbk\n",
      "Stock tcbk most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in chdn\n",
      "Stock chdn most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in rop\n",
      "Stock rop most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in pej\n",
      "Stock pej most recent date is 2020-06-12 00:00:00 oldest date is 2005-06-23 00:00:00. Skipping it\n",
      "Reading in are\n",
      "Stock are most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in ntrs\n",
      "Stock ntrs most recent date is 2020-06-05 00:00:00 oldest date is 1990-03-26 00:00:00. Skipping it\n",
      "Reading in rbb\n",
      "Stock rbb most recent date is 2020-06-05 00:00:00 oldest date is 2017-07-26 00:00:00. Skipping it\n",
      "Reading in dia\n",
      "Stock dia most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in key\n",
      "Stock key most recent date is 2020-06-05 00:00:00 oldest date is 1987-11-05 00:00:00. Skipping it\n",
      "Reading in pg\n",
      "Reading in ibuy\n",
      "Stock ibuy most recent date is 2020-05-29 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in tjx\n",
      "Reading in ftc\n",
      "Stock ftc most recent date is 2020-05-29 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in bsrr\n",
      "Stock bsrr most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in irm\n",
      "Stock irm most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in lb\n",
      "Reading in doyu\n",
      "Stock doyu most recent date is 2020-06-05 00:00:00 oldest date is 2019-07-17 00:00:00. Skipping it\n",
      "Reading in frc\n",
      "Stock frc most recent date is 2020-06-05 00:00:00 oldest date is 2010-12-13 00:00:00. Skipping it\n",
      "Reading in pub\n",
      "Stock pub most recent date is 2020-06-05 00:00:00 oldest date is 2015-06-11 00:00:00. Skipping it\n",
      "Reading in pvh\n",
      "Stock pvh most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in caty\n",
      "Stock caty most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in mlco\n",
      "Stock mlco most recent date is 2020-06-05 00:00:00 oldest date is 2006-12-18 00:00:00. Skipping it\n",
      "Reading in pacw\n",
      "Stock pacw most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in pld\n",
      "Stock pld most recent date is 2020-06-05 00:00:00 oldest date is 1995-08-18 00:00:00. Skipping it\n",
      "Reading in agnc\n",
      "Stock agnc most recent date is 2020-06-05 00:00:00 oldest date is 2008-05-14 00:00:00. Skipping it\n",
      "Reading in czr\n",
      "Stock czr most recent date is 2020-06-05 00:00:00 oldest date is 2012-02-10 00:00:00. Skipping it\n",
      "Reading in sui\n",
      "Stock sui most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in adbe\n",
      "Reading in ibuy\n",
      "Stock ibuy most recent date is 2020-06-05 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in peg\n",
      "Reading in c\n",
      "Stock c most recent date is 2020-05-29 00:00:00 oldest date is 1970-01-02 00:00:00. Skipping it\n",
      "Reading in pfbc\n",
      "Stock pfbc most recent date is 2020-06-05 00:00:00 oldest date is 2011-07-21 00:00:00. Skipping it\n",
      "Reading in zion\n",
      "Stock zion most recent date is 2020-06-05 00:00:00 oldest date is 1990-03-26 00:00:00. Skipping it\n",
      "Reading in cwbc\n",
      "Stock cwbc most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in atvi\n",
      "Stock atvi most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in lamr\n",
      "Stock lamr most recent date is 2020-06-05 00:00:00 oldest date is 1996-08-02 00:00:00. Skipping it\n",
      "Reading in emr\n",
      "Reading in inn\n",
      "Stock inn most recent date is 2020-05-29 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in doyu\n",
      "Stock doyu most recent date is 2020-06-05 00:00:00 oldest date is 2019-07-17 00:00:00. Skipping it\n",
      "Reading in hasi\n",
      "Stock hasi most recent date is 2020-06-05 00:00:00 oldest date is 2013-04-18 00:00:00. Skipping it\n",
      "Reading in ce\n",
      "Stock ce most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in eri\n",
      "Stock eri most recent date is 2020-06-05 00:00:00 oldest date is 2014-09-22 00:00:00. Skipping it\n",
      "Reading in ubfo\n",
      "Stock ubfo most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in jpm\n",
      "Reading in boch\n",
      "Stock boch most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in vfh\n",
      "Stock vfh most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in iwm\n",
      "Stock iwm most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in wabc\n",
      "Stock wabc most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in vno\n",
      "Stock vno most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in gbci\n",
      "Stock gbci most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in znga\n",
      "Stock znga most recent date is 2020-06-05 00:00:00 oldest date is 2011-12-20 00:00:00. Skipping it\n",
      "Reading in cvbf\n",
      "Stock cvbf most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in dlr\n",
      "Stock dlr most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in pnc\n",
      "Stock pnc most recent date is 2020-06-05 00:00:00 oldest date is 1988-09-07 00:00:00. Skipping it\n",
      "Reading in hafc\n",
      "Stock hafc most recent date is 2020-06-05 00:00:00 oldest date is 2011-12-20 00:00:00. Skipping it\n",
      "Reading in umpq\n",
      "Stock umpq most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in mgm\n",
      "Stock mgm most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in pei\n",
      "Stock pei most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in lvs\n",
      "Stock lvs most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in well\n",
      "Stock well most recent date is 2020-06-05 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in mtb\n",
      "Stock mtb most recent date is 2020-06-05 00:00:00 oldest date is 1991-10-04 00:00:00. Skipping it\n",
      "Reading in dre\n",
      "Stock dre most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in rrr\n",
      "Stock rrr most recent date is 2020-06-05 00:00:00 oldest date is 2016-04-27 00:00:00. Skipping it\n",
      "Reading in eqix\n",
      "Stock eqix most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in bmrc\n",
      "Stock bmrc most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in spy\n",
      "Stock spy most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in stt\n",
      "Stock stt most recent date is 2020-06-05 00:00:00 oldest date is 1986-07-09 00:00:00. Skipping it\n",
      "Reading in rost\n",
      "Reading in fhb\n",
      "Stock fhb most recent date is 2020-06-05 00:00:00 oldest date is 2016-08-04 00:00:00. Skipping it\n",
      "Reading in sivb\n",
      "Stock sivb most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in reg\n",
      "Stock reg most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in ibuy\n",
      "Stock ibuy most recent date is 2020-06-12 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in so\n",
      "Reading in srg\n",
      "Stock srg most recent date is 2020-06-05 00:00:00 oldest date is 2015-07-06 00:00:00. Skipping it\n",
      "Reading in fsbw\n",
      "Stock fsbw most recent date is 2020-06-05 00:00:00 oldest date is 2012-07-10 00:00:00. Skipping it\n",
      "Reading in qqq\n",
      "Stock qqq most recent date is 2020-06-12 00:00:00 oldest date is 1999-03-10 00:00:00. Skipping it\n",
      "Reading in byd\n",
      "Stock byd most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in cpt\n",
      "Stock cpt most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in ewbc\n",
      "Stock ewbc most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in bcml\n",
      "Stock bcml most recent date is 2020-06-05 00:00:00 oldest date is 2017-01-03 00:00:00. Skipping it\n",
      "Reading in igt\n",
      "Stock igt most recent date is 2020-06-05 00:00:00 oldest date is 2015-04-07 00:00:00. Skipping it\n",
      "Reading in cma\n",
      "Stock cma most recent date is 2020-06-05 00:00:00 oldest date is 1990-03-26 00:00:00. Skipping it\n",
      "Reading in wal\n",
      "Stock wal most recent date is 2020-06-05 00:00:00 oldest date is 2005-06-29 00:00:00. Skipping it\n",
      "Reading in hiw\n",
      "Stock hiw most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in vti\n",
      "Stock vti most recent date is 2020-06-12 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in wynn\n",
      "Stock wynn most recent date is 2020-06-05 00:00:00 oldest date is 2005-02-25 00:00:00. Skipping it\n",
      "Reading in pbj\n",
      "Stock pbj most recent date is 2020-06-12 00:00:00 oldest date is 2005-06-23 00:00:00. Skipping it\n",
      "Reading in ccb\n",
      "Stock ccb most recent date is 2020-06-05 00:00:00 oldest date is 2018-07-18 00:00:00. Skipping it\n",
      "Reading in usb\n",
      "Stock usb most recent date is 2020-06-05 00:00:00 oldest date is 1987-11-05 00:00:00. Skipping it\n"
     ]
    }
   ],
   "source": [
    "dict_of_stocks_and_dfs = read_in_data(recent_date_check=datetime.datetime(2020,6,12).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_of_stocks_and_dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Correlation Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_correlation_dfs(dict_of_stocks_and_dfs, n_day_rolling_features_list=[ 3,5, 7, 10, 30, 180, 365], verbose=False):\n",
    "    \"\"\"\n",
    "    Create correlation + variance based  upon daily closing stock prices for given date ranges\n",
    "    \n",
    "    also include daily volume\n",
    "    \n",
    "    We are trying to  predict 7 day correaltion\n",
    "    \"\"\"\n",
    "\n",
    "    stock_features_dict = defaultdict(pd.DataFrame)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    start = time.time()\n",
    "    n_stocks = len(dict_of_stocks_and_dfs.keys())\n",
    "    final_feature_df = create_date_dummy_df()\n",
    "    pairs_of_stocks = []\n",
    "    \n",
    "    for idx, first_stock_name in enumerate(dict_of_stocks_and_dfs.keys()):\n",
    "        print('')\n",
    "        print(f\"Finished {idx/n_stocks} pct of stocks\")\n",
    "        print('')\n",
    "        for second_idx, second_stock_name in enumerate(dict_of_stocks_and_dfs.keys()):\n",
    "            stock_pair = f\"{first_stock_name}_{second_stock_name}\"\n",
    "            reverse_pair = f\"{second_stock_name}_{first_stock_name}\"\n",
    "            \n",
    "            if (first_stock_name == second_stock_name) or (stock_pair in pairs_of_stocks)  or (reverse_pair in pairs_of_stocks): # pnr -> ual same as ual -> pnr\n",
    "                continue\n",
    "            else:\n",
    "                pairs_of_stocks.append(stock_pair)\n",
    "            if verbose:\n",
    "                print('-------')\n",
    "                print(f\"{first_stock_name} & {second_stock_name}\")\n",
    "                print('-------')\n",
    "            \n",
    "            # here the date is not the index, yet\n",
    "            first_stock_df = dict_of_stocks_and_dfs[f\"{first_stock_name}\"].loc[ \n",
    "                dict_of_stocks_and_dfs[f\"{first_stock_name}\"].date.isin(dict_of_stocks_and_dfs[f\"{second_stock_name}\"].date), :]\n",
    "\n",
    "            #  filter second df by the dates in first\n",
    "\n",
    "            # here the date is not the index, yet\n",
    "            second_stock_df = dict_of_stocks_and_dfs[f\"{second_stock_name}\"].loc[ \n",
    "                dict_of_stocks_and_dfs[f\"{second_stock_name}\"].date.isin(first_stock_df.date), :]\n",
    "            \n",
    "            # set the date as an index and sort by date\n",
    "            first_stock_df = first_stock_df.sort_values('date')\n",
    "            second_stock_df = second_stock_df.sort_values('date')\n",
    "\n",
    "            first_stock_df = first_stock_df.set_index('date')\n",
    "            second_stock_df = second_stock_df.set_index('date')\n",
    "            \n",
    "            all_features_df = pd.DataFrame()\n",
    "            for rolling_idx, rolling_day in enumerate(n_day_rolling_features_list):\n",
    "                if verbose:\n",
    "                    print(f\"Rolling calculations for {rolling_day}\")\n",
    "                features_df = create_correlation_and_variance_features(\n",
    "                    first_stock_df, second_stock_df, rolling_day, final_feature_df, \n",
    "                    first_stock_name=first_stock_name, second_stock_name=second_stock_name)\n",
    "                   \n",
    "                current_feature_cols = set(features_df.columns)\n",
    "                final_feature_cols = set(final_feature_df.columns)\n",
    "\n",
    "                \n",
    "                if (f\"{first_stock_name}_volume\" not in final_feature_df.columns) and (rolling_idx == 0):\n",
    "                    features_df[f\"{first_stock_name}_volume\"] = list(first_stock_df.volume)\n",
    "                \n",
    "                if (f\"{second_stock_name}_volume\" not in final_feature_df.columns) and (rolling_idx == 0):\n",
    "                    features_df[f\"{second_stock_name}_volume\"] = list(second_stock_df.volume)\n",
    "                    \n",
    "                if rolling_idx == 0: \n",
    "                    all_features_df = features_df\n",
    "                else:\n",
    "                    all_features_df = all_features_df.join(features_df, on='date', lsuffix='_left')\n",
    "            \n",
    "\n",
    "                    \n",
    "            all_features_df.index = pd.to_datetime(all_features_df.index)\n",
    "            final_feature_df = final_feature_df.join(all_features_df, on='date')\n",
    "\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(f\"Building all features took {(end-start)/60} minutes\")\n",
    "                start = time.time()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total time {(end_time-start_time) / 60} minutes for {len(pairs_of_stocks)} pairs\")\n",
    "    final_feature_df = add_time_feature(final_feature_df)\n",
    "    return final_feature_df, pairs_of_stocks\n",
    "            \n",
    "        \n",
    "\n",
    "# Note: will eventuall need to add in 0s for stocks withour correlation data with other stocks due to date range\n",
    "def create_date_dummy_df(start_date=datetime.datetime(1980,1,1), n_years=50):\n",
    "    \n",
    "    #  create dummy df with dates to join against\n",
    "    list_of_dates  = []\n",
    "    n_days = 365*n_years\n",
    "    start_date = start_date\n",
    "\n",
    "    for i in range(n_days):\n",
    "        list_of_dates.append(start_date + datetime.timedelta(i))\n",
    "    df_ = pd.DataFrame(list_of_dates, columns=['date'])\n",
    "    \n",
    "    df_.date_ =  pd.to_datetime(df_.date)\n",
    "    return df_.set_index('date')\n",
    "\n",
    "def add_time_feature(final_stock_df):\n",
    "    \n",
    "    days = [i.day for i in final_stock_df.index]\n",
    "    months = [i.month for i in final_stock_df.index]\n",
    "    quarters = [i.quarter for i in final_stock_df.index]\n",
    "    years = [i.year for i in final_stock_df.index]\n",
    "    \n",
    "    us_holidays = holidays.UnitedStates()\n",
    "    \n",
    "    h_ = np.array([i in us_holidays for i in final_stock_df.index]).astype(int)\n",
    "\n",
    "\n",
    "    final_stock_df['day'] = days\n",
    "    final_stock_df['month'] = months\n",
    "    final_stock_df['quarter'] = quarters\n",
    "    final_stock_df['year'] = years\n",
    "#     final_stock_df['is_holiday'] = h_\n",
    "    \n",
    "    return final_stock_df\n",
    "\n",
    "def create_correlation_and_variance_features(first_stock_df, second_stock_df, n_days_stride, final_stock_df, \n",
    "                                             first_stock_name=None, second_stock_name=None, verbose=False):\n",
    "    \"\"\"\n",
    "    n_days_stride: the  number of rolling days to calculate correlation for\n",
    "    \"\"\"\n",
    "    n_rows = len(first_stock_df)\n",
    "\n",
    "    previous_row = 0\n",
    "\n",
    "    features_per_time_period = defaultdict(list)\n",
    "    if verbose:\n",
    "        print(f\"Creating correlations + variance on close for {n_days_stride} days\")\n",
    "    \n",
    "    rolling_close_df = pd.DataFrame(first_stock_df.close.rolling(\n",
    "        n_days_stride).corr(second_stock_df.close)).rename(\n",
    "        {'close': f\"{first_stock_name}_{second_stock_name}_close_corr_rolling_{n_days_stride}_days\"},axis=1).fillna(method='backfill').round(6)\n",
    "\n",
    "    \n",
    "    # add cols\n",
    "    \n",
    "    current_feature_cols = list(final_stock_df.columns)\n",
    "    \n",
    "\n",
    "    # as we go through different pairs will have multiple var / corr for the first stock\n",
    "    # pnc_bar calcualtes corr for pnr\n",
    "    #pnr_bat calculates corr for pnr\n",
    "    # don't want the same cols\n",
    "    if f\"{first_stock_name}_close_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        \n",
    "        rolling_close_std_first_stock =  first_stock_df.close.rolling(n_days_stride).std().fillna(method='backfill').round(6)\n",
    "        rolling_close_df[f\"{first_stock_name}_close_std_rolling_{n_days_stride}_days\"] = rolling_close_std_first_stock\n",
    "        \n",
    "    if f\"{second_stock_name}_close_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        rolling_close_std_second_stock =  second_stock_df.close.rolling(n_days_stride).std().fillna(method='backfill').round( 6)\n",
    "        rolling_close_df[f\"{second_stock_name}_close_std_rolling_{n_days_stride}_days\"] = rolling_close_std_second_stock\n",
    "        \n",
    "    if f\"{first_stock_name}_volume_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        rolling_volume_std_first_stock =  first_stock_df.volume.rolling(n_days_stride).std().fillna(method='backfill').round(6)\n",
    "        rolling_close_df[f\"{first_stock_name}_volume_std_rolling_{n_days_stride}_days\"] = rolling_volume_std_first_stock\n",
    "        \n",
    "    if f\"{second_stock_name}_volume_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        rolling_volume_std_second_stock =  second_stock_df.volume.rolling(n_days_stride).std().fillna(method='backfill').round(6)\n",
    "        rolling_close_df[f\"{second_stock_name}_volume_std_rolling_{n_days_stride}_days\"] = rolling_volume_std_second_stock\n",
    "    \n",
    "    return rolling_close_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished 0.0 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.07692307692307693 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.15384615384615385 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.23076923076923078 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.3076923076923077 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.38461538461538464 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.46153846153846156 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.5384615384615384 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.6153846153846154 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.6923076923076923 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.7692307692307693 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.8461538461538461 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.9230769230769231 pct of stocks\n",
      "\n",
      "Total time 0.39467519919077554 minutes for 78 pairs\n"
     ]
    }
   ],
   "source": [
    "# 2 minutes fo 210 pairs\n",
    "final_stock_df, pairs_of_stocks = build_correlation_dfs(dict_of_stocks_and_dfs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep NN Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for LSTM model\n",
    "def split_sequences(sequences, n_steps, y_col='pg_so_close_corr_rolling_7_days', \n",
    "                    start_idx=0, n_val=50, print_idx=100, input_verbose=1,     rank=100): #2200\n",
    "    \"\"\"\n",
    "    sequences = input_data\n",
    "    n_steps = n_days of data to give at a time\n",
    "    \n",
    "    only works for the currently set y_col\n",
    "    \"\"\"\n",
    "    if y_col not in sequences.columns:\n",
    "        raise ValueError('This y col does not exist in this df')\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    X_val, y_val = list(), list()\n",
    "    \n",
    "    n_sequences = len(sequences)\n",
    "    print('n_sequences', n_sequences)\n",
    "\n",
    "    for i in range(start_idx, n_sequences):\n",
    "        if i == start_idx and input_verbose == 1:\n",
    "            print(f\"Training idx start at {i}\")\n",
    "        if (i % print_idx == 0) and (i != 0) and input_verbose==1:\n",
    "            print(f\"Pct finished = {i/n_sequences}\")\n",
    "            \n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps \n",
    "        total_end_ix = end_ix + n_val\n",
    "        # check if we are beyond the dataset\n",
    "        if (total_end_ix) > n_sequences:\n",
    "            print(f\"Training idx end at {end_ix}\")\n",
    "            print('Total idx checked', total_end_ix)\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = np.array(sequences.loc[:, sequences.columns != f\"{y_col}\"][i:end_ix]), np.array(\n",
    "            sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix-1])\n",
    "\n",
    "                                 \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    val_start_idx = start_idx + n_sequences - (start_idx  + n_val -2)\n",
    "    for i in range(val_start_idx, n_sequences):\n",
    "        if i == val_start_idx and input_verbose==1:\n",
    "            print(f\"Val idx start at {val_start_idx}\")\n",
    "        if (i % print_idx == 0) and i != 0 and input_verbose==1:\n",
    "            print(f\"Pct finished for val sequences = {i/n_sequences}\")\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences) and input_verbose==1:\n",
    "            print(f\"Val idx end at {end_ix}\")\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = np.array(sequences.loc[:, sequences.columns != f\"{y_col}\"][i:end_ix]), np.array(\n",
    "            sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix-1])\n",
    "        \n",
    "        \n",
    "        X_val.append(seq_x)\n",
    "        y_val.append(seq_y)\n",
    "    \n",
    "    \n",
    "\n",
    "    X, y, X_val, y_val = array(X), array(y), array(X_val), array(y_val)\n",
    "    \n",
    "\n",
    "    \n",
    "    # errors for standard scaler\n",
    "    X = np.nan_to_num(X.astype(np.float32)) # converting to float 32 throws some infinity errors\n",
    "    X_val = np.nan_to_num(X_val.astype(np.float32)) # converting to float 32 throws some infinity errors  \n",
    "    \n",
    "    return X, y, X_val, y_val\n",
    "    \n",
    "    scalers = {}\n",
    "    for i in range(X.shape[1]):\n",
    "        scalers[i] = StandardScaler()\n",
    "        X[:, i, :] = scalers[i].fit_transform(X[:, i, :]) \n",
    "    X = np.nan_to_num(X.astype(np.float32)) # converting to float 32 throws some infinity errors\n",
    "    \n",
    "    pca_scalers = {}\n",
    "\n",
    "    new_X = np.zeros((X.shape[0], X.shape[1], n_pca_components))\n",
    "    for i in range(X.shape[1]):\n",
    "        pca_scalers[i] = PCA(n_components=n_pca_components) # ~80%\n",
    "        new_X[:, i, :] = pca_scalers[i].fit_transform(X[:, i, :]) \n",
    "\n",
    "\n",
    "    for i in range(X_val.shape[1]):\n",
    "        X_val[:, i, :] = scalers[i].transform(X_val[:, i, :]) \n",
    "\n",
    "    X_val = np.nan_to_num(X_val.astype(np.float32)) # converting to float 32 throws some infinity errors  \n",
    "\n",
    "    new_X_val = np.zeros((X_val.shape[0], X_val.shape[1], n_pca_components))\n",
    "    for i in range(X_val.shape[1]):\n",
    "        new_X_val[:, i, :] = pca_scalers[i].transform(X_val[:, i, :]) \n",
    "        \n",
    "   # need  to do this again as standard scaler may have nans\n",
    "    X = np.nan_to_num(X.astype(np.float32)) # converting to float 32 throws some infinity errors\n",
    "    X_val = np.nan_to_num(X_val.astype(np.float32)) # converting to float 32 throws some infinity errors \n",
    "    print('X val shape', X_val.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    return new_X, y, new_X_val, y_val, scalers, pca_scalers\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stock_df = final_stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sequences 616\n",
      "Training idx start at 0\n",
      "Pct finished = 0.16233766233766234\n",
      "Pct finished = 0.3246753246753247\n",
      "Pct finished = 0.487012987012987\n",
      "Pct finished = 0.6493506493506493\n",
      "Pct finished = 0.8116883116883117\n",
      "Training idx end at 587\n",
      "Total idx checked 617\n",
      "Val idx start at 588\n",
      "Val idx end at 618\n"
     ]
    }
   ],
   "source": [
    "X, y, X_val, y_val = split_sequences(final_stock_df[final_stock_df.index>='2018-01-01'],\n",
    "                                     30,  n_val=30, y_col=\"rost_so_close_corr_rolling_7_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "core, factors = tl.decomposition.tucker(X,[100,100,X.shape[0]])  \n",
    "end = time.time()\n",
    "print(f\"Time in mins = {(end-start)/60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8112, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daily-trading",
   "language": "python",
   "name": "daily-trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
