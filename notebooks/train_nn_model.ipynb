{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization, TimeDistributed, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from multiprocessing import Process, Pool\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLIEST_DATE = '2012-01-01'\n",
    "N_COMPONENTS=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REad in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_in_data(check_recent_date=True, recent_date_check=datetime.datetime.now().date()):\n",
    "    dict_of_stocks_and_dfs = {}\n",
    "    for file_ in glob.glob('../data/updated_historical_stock_and_etf_data/*.csv'):\n",
    "        stock_name = file_.rsplit(\"/\")[-1].split('_')[0].lower() \n",
    "        print(f\"Reading in {stock_name}\")\n",
    "        df_  = pd.read_csv(f\"{file_}\")\n",
    "        # ensure we have the most recent data\n",
    "        most_recent_date = pd.to_datetime(df_.date.max())\n",
    "        oldest_date = pd.to_datetime(df_.date.min())\n",
    "        \n",
    "        oldest_date_bool = oldest_date < datetime.datetime(2006,1,1).date()\n",
    "        recent_date_bool = most_recent_date == recent_date_check\n",
    "        \n",
    "        if oldest_date_bool and recent_date_bool:\n",
    "            dict_of_stocks_and_dfs[stock_name] = df_.sort_values('date')\n",
    "        elif oldest_date_bool and not check_recent_date:\n",
    "            dict_of_stocks_and_dfs[stock_name] = df_.sort_values('date')            \n",
    "        else:\n",
    "            print(f\"Stock {stock_name} most recent date is {most_recent_date} oldest date is {oldest_date}. Skipping it\")\n",
    "    return dict_of_stocks_and_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in wal\n",
      "Reading in fsbw\n",
      "Stock fsbw most recent date is 2020-05-15 00:00:00 oldest date is 2012-07-10 00:00:00. Skipping it\n",
      "Reading in pfbc\n",
      "Stock pfbc most recent date is 2020-05-15 00:00:00 oldest date is 2011-07-21 00:00:00. Skipping it\n",
      "Reading in hasi\n",
      "Stock hasi most recent date is 2020-05-15 00:00:00 oldest date is 2013-04-18 00:00:00. Skipping it\n",
      "Reading in mtb\n",
      "Reading in rbb\n",
      "Stock rbb most recent date is 2020-05-15 00:00:00 oldest date is 2017-07-26 00:00:00. Skipping it\n",
      "Reading in jpm\n",
      "Reading in umpq\n",
      "Reading in cvbf\n",
      "Reading in fitb\n",
      "Reading in irm\n",
      "Reading in wfc\n",
      "Reading in cwbc\n",
      "Reading in bsrr\n",
      "Reading in key\n",
      "Reading in ewbc\n",
      "Reading in dlr\n",
      "Reading in ubfo\n",
      "Reading in cvcy\n",
      "Reading in hope\n",
      "Stock hope most recent date is 2020-05-15 00:00:00 oldest date is 2011-12-05 00:00:00. Skipping it\n",
      "Reading in pacw\n",
      "Reading in ntrs\n",
      "Reading in agnc\n",
      "Stock agnc most recent date is 2020-05-15 00:00:00 oldest date is 2008-05-14 00:00:00. Skipping it\n",
      "Reading in cma\n",
      "Reading in nrz\n",
      "Stock nrz most recent date is 2020-05-15 00:00:00 oldest date is 2013-05-02 00:00:00. Skipping it\n",
      "Reading in gbci\n",
      "Reading in hiw\n",
      "Reading in sui\n",
      "Reading in c\n",
      "Reading in colb\n",
      "Reading in srg\n",
      "Stock srg most recent date is 2020-05-15 00:00:00 oldest date is 2015-07-06 00:00:00. Skipping it\n",
      "Reading in dre\n",
      "Reading in lamr\n",
      "Reading in wabc\n",
      "Reading in bac\n",
      "Reading in bku\n",
      "Stock bku most recent date is 2020-05-15 00:00:00 oldest date is 2011-02-01 00:00:00. Skipping it\n",
      "Reading in bmrc\n",
      "Reading in vno\n",
      "Reading in usb\n",
      "Reading in caty\n",
      "Reading in sivb\n",
      "Reading in boch\n",
      "Reading in stt\n",
      "Reading in pnc\n",
      "Reading in are\n",
      "Reading in cpg\n",
      "Stock cpg most recent date is 2020-05-15 00:00:00 oldest date is 2006-07-21 00:00:00. Skipping it\n",
      "Reading in frc\n",
      "Stock frc most recent date is 2020-05-15 00:00:00 oldest date is 2010-12-13 00:00:00. Skipping it\n",
      "Reading in pei\n",
      "Reading in cpt\n",
      "Reading in reg\n"
     ]
    }
   ],
   "source": [
    "dict_of_stocks_and_dfs = read_in_data(recent_date_check=datetime.datetime(2020,5,15).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1482"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(dict_of_stocks_and_dfs.keys())-1) * len(dict_of_stocks_and_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_correlation_dfs(dict_of_stocks_and_dfs, n_day_rolling_features_list=[ 5, 7, 10, 30, 180, 365], verbose=False):\n",
    "    \"\"\"\n",
    "    Create correlation + variance based  upon daily closing stock prices for given date ranges\n",
    "    \n",
    "    also include daily volume\n",
    "    \n",
    "    We are trying to  predict 7 day correaltion\n",
    "    \"\"\"\n",
    "\n",
    "    stock_features_dict = defaultdict(pd.DataFrame)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    start = time.time()\n",
    "    n_stocks = len(dict_of_stocks_and_dfs.keys())\n",
    "    final_feature_df = create_date_dummy_df()\n",
    "    pairs_of_stocks = []\n",
    "    \n",
    "    for idx, first_stock_name in enumerate(dict_of_stocks_and_dfs.keys()):\n",
    "        print('')\n",
    "        print(f\"Finished {idx/n_stocks} pct of stocks\")\n",
    "        print('')\n",
    "        for second_idx, second_stock_name in enumerate(dict_of_stocks_and_dfs.keys()):\n",
    "            stock_pair = f\"{first_stock_name}_{second_stock_name}\"\n",
    "            reverse_pair = f\"{second_stock_name}_{first_stock_name}\"\n",
    "            \n",
    "            if (first_stock_name == second_stock_name) or (stock_pair in pairs_of_stocks)  or (reverse_pair in pairs_of_stocks): # pnr -> ual same as ual -> pnr\n",
    "                continue\n",
    "            else:\n",
    "                pairs_of_stocks.append(stock_pair)\n",
    "            if verbose:\n",
    "                print('-------')\n",
    "                print(f\"{first_stock_name} & {second_stock_name}\")\n",
    "                print('-------')\n",
    "            \n",
    "            # here the date is not the index, yet\n",
    "            first_stock_df = dict_of_stocks_and_dfs[f\"{first_stock_name}\"].loc[ \n",
    "                dict_of_stocks_and_dfs[f\"{first_stock_name}\"].date.isin(dict_of_stocks_and_dfs[f\"{second_stock_name}\"].date), :]\n",
    "\n",
    "            #  filter second df by the dates in first\n",
    "\n",
    "            # here the date is not the index, yet\n",
    "            second_stock_df = dict_of_stocks_and_dfs[f\"{second_stock_name}\"].loc[ \n",
    "                dict_of_stocks_and_dfs[f\"{second_stock_name}\"].date.isin(first_stock_df.date), :]\n",
    "            \n",
    "            # set the date as an index and sort by date\n",
    "            first_stock_df = first_stock_df.sort_values('date')\n",
    "            second_stock_df = second_stock_df.sort_values('date')\n",
    "\n",
    "            first_stock_df = first_stock_df.set_index('date')\n",
    "            second_stock_df = second_stock_df.set_index('date')\n",
    "            \n",
    "            all_features_df = pd.DataFrame()\n",
    "            for rolling_idx, rolling_day in enumerate(n_day_rolling_features_list):\n",
    "                if verbose:\n",
    "                    print(f\"Rolling calculations for {rolling_day}\")\n",
    "                features_df = create_correlation_and_variance_features(\n",
    "                    first_stock_df, second_stock_df, rolling_day, final_feature_df, \n",
    "                    first_stock_name=first_stock_name, second_stock_name=second_stock_name)\n",
    "                   \n",
    "                current_feature_cols = set(features_df.columns)\n",
    "                final_feature_cols = set(final_feature_df.columns)\n",
    "\n",
    "                \n",
    "                if (f\"{first_stock_name}_volume\" not in final_feature_df.columns) and (rolling_idx == 0):\n",
    "                    features_df[f\"{first_stock_name}_volume\"] = list(first_stock_df.volume)\n",
    "                \n",
    "                if (f\"{second_stock_name}_volume\" not in final_feature_df.columns) and (rolling_idx == 0):\n",
    "                    features_df[f\"{second_stock_name}_volume\"] = list(second_stock_df.volume)\n",
    "                    \n",
    "                if rolling_idx == 0: \n",
    "                    all_features_df = features_df\n",
    "                else:\n",
    "                    all_features_df = all_features_df.join(features_df, on='date', lsuffix='_left')\n",
    "            \n",
    "\n",
    "                    \n",
    "            all_features_df.index = pd.to_datetime(all_features_df.index)\n",
    "            final_feature_df = final_feature_df.join(all_features_df, on='date')\n",
    "\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(f\"Building all features took {(end-start)/60} minutes\")\n",
    "                start = time.time()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total time {(end_time-start_time) / 60} minutes for {len(pairs_of_stocks)} pairs\")\n",
    "    final_feature_df = add_time_feature(final_feature_df)\n",
    "    return final_feature_df, pairs_of_stocks\n",
    "            \n",
    "        \n",
    "\n",
    "# Note: will eventuall need to add in 0s for stocks withour correlation data with other stocks due to date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_dummy_df(start_date=datetime.datetime(1980,1,1), n_years=50):\n",
    "    \n",
    "    #  create dummy df with dates to join against\n",
    "    list_of_dates  = []\n",
    "    n_days = 365*n_years\n",
    "    start_date = start_date\n",
    "\n",
    "    for i in range(n_days):\n",
    "        list_of_dates.append(start_date + datetime.timedelta(i))\n",
    "    df_ = pd.DataFrame(list_of_dates, columns=['date'])\n",
    "    \n",
    "    df_.date_ =  pd.to_datetime(df_.date)\n",
    "    return df_.set_index('date')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_feature(final_stock_df):\n",
    "    \n",
    "    days = [i.day for i in final_stock_df.index]\n",
    "    months = [i.month for i in final_stock_df.index]\n",
    "    quarters = [i.quarter for i in final_stock_df.index]\n",
    "    years = [i.year for i in final_stock_df.index]\n",
    "    \n",
    "    us_holidays = holidays.UnitedStates()\n",
    "    \n",
    "    h_ = np.array([i in us_holidays for i in final_stock_df.index]).astype(int)\n",
    "\n",
    "\n",
    "    final_stock_df['day'] = days\n",
    "    final_stock_df['month'] = months\n",
    "    final_stock_df['quarter'] = quarters\n",
    "    final_stock_df['year'] = years\n",
    "#     final_stock_df['is_holiday'] = h_\n",
    "    \n",
    "    return final_stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_and_variance_features(first_stock_df, second_stock_df, n_days_stride, final_stock_df, \n",
    "                                             first_stock_name=None, second_stock_name=None, verbose=False):\n",
    "    \"\"\"\n",
    "    n_days_stride: the  number of rolling days to calculate correlation for\n",
    "    \"\"\"\n",
    "    n_rows = len(first_stock_df)\n",
    "\n",
    "    previous_row = 0\n",
    "\n",
    "    features_per_time_period = defaultdict(list)\n",
    "    if verbose:\n",
    "        print(f\"Creating correlations + variance on close for {n_days_stride} days\")\n",
    "    \n",
    "    rolling_close_df = pd.DataFrame(first_stock_df.close.rolling(\n",
    "        n_days_stride).corr(second_stock_df.close)).rename(\n",
    "        {'close': f\"{first_stock_name}_{second_stock_name}_close_corr_rolling_{n_days_stride}_days\"},axis=1).fillna(method='backfill').round(6)\n",
    "\n",
    "    \n",
    "    # add cols\n",
    "    \n",
    "    current_feature_cols = list(final_stock_df.columns)\n",
    "    \n",
    "\n",
    "    # as we go through different pairs will have multiple var / corr for the first stock\n",
    "    # pnc_bar calcualtes corr for pnr\n",
    "    #pnr_bat calculates corr for pnr\n",
    "    # don't want the same cols\n",
    "    if f\"{first_stock_name}_close_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        \n",
    "        rolling_close_std_first_stock =  first_stock_df.close.rolling(n_days_stride).std().fillna(method='backfill').round(6)\n",
    "        rolling_close_df[f\"{first_stock_name}_close_std_rolling_{n_days_stride}_days\"] = rolling_close_std_first_stock\n",
    "        \n",
    "    if f\"{second_stock_name}_close_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        rolling_close_std_second_stock =  second_stock_df.close.rolling(n_days_stride).std().fillna(method='backfill').round( 6)\n",
    "        rolling_close_df[f\"{second_stock_name}_close_std_rolling_{n_days_stride}_days\"] = rolling_close_std_second_stock\n",
    "        \n",
    "    if f\"{first_stock_name}_volume_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        rolling_volume_std_first_stock =  first_stock_df.volume.rolling(n_days_stride).std().fillna(method='backfill').round(6)\n",
    "        rolling_close_df[f\"{first_stock_name}_volume_std_rolling_{n_days_stride}_days\"] = rolling_volume_std_first_stock\n",
    "        \n",
    "    if f\"{second_stock_name}_volume_std_rolling_{n_days_stride}_days\" not in current_feature_cols:\n",
    "        rolling_volume_std_second_stock =  second_stock_df.volume.rolling(n_days_stride).std().fillna(method='backfill').round(6)\n",
    "        rolling_close_df[f\"{second_stock_name}_volume_std_rolling_{n_days_stride}_days\"] = rolling_volume_std_second_stock\n",
    "    \n",
    "    return rolling_close_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished 0.0 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.02564102564102564 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.05128205128205128 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.07692307692307693 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.10256410256410256 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.1282051282051282 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.15384615384615385 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.1794871794871795 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.20512820512820512 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.23076923076923078 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.2564102564102564 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.28205128205128205 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.3076923076923077 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.3333333333333333 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.358974358974359 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.38461538461538464 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.41025641025641024 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.4358974358974359 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.46153846153846156 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.48717948717948717 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.5128205128205128 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.5384615384615384 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.5641025641025641 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.5897435897435898 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.6153846153846154 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.6410256410256411 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.6666666666666666 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.6923076923076923 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.717948717948718 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.7435897435897436 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.7692307692307693 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.7948717948717948 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.8205128205128205 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.8461538461538461 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.8717948717948718 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.8974358974358975 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.9230769230769231 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.9487179487179487 pct of stocks\n",
      "\n",
      "\n",
      "Finished 0.9743589743589743 pct of stocks\n",
      "\n",
      "Total time 5.419573684533437 minutes for 741 pairs\n"
     ]
    }
   ],
   "source": [
    "# 2 minutes fo 210 pairs\n",
    "final_stock_df, pairs_of_stocks = build_correlation_dfs(dict_of_stocks_and_dfs, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of stocks is ~300 NOT 990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4957"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_stock_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5777"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_stock_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep code for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for LSTM model\n",
    "def split_sequences(sequences, n_steps, y_col='pg_so_close_corr_rolling_7_days', start_idx=0, n_val=50, print_idx=100, input_verbose=1): #2200\n",
    "    \"\"\"\n",
    "    sequences = input_data\n",
    "    n_steps = n_days of data to give at a time\n",
    "    \n",
    "    only works for the currently set y_col\n",
    "    \"\"\"\n",
    "    if y_col not in sequences.columns:\n",
    "        raise ValueError('This y col does not exist in this df')\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    X_val, y_val = list(), list()\n",
    "    \n",
    "    n_sequences = len(sequences)\n",
    "    print('n_sequences', n_sequences)\n",
    "\n",
    "    for i in range(start_idx, n_sequences):\n",
    "        if i == start_idx and input_verbose == 1:\n",
    "            print(f\"Training idx start at {i}\")\n",
    "        if (i % print_idx == 0) and (i != 0) and input_verbose==1:\n",
    "            print(f\"Pct finished = {i/n_sequences}\")\n",
    "            \n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps \n",
    "        total_end_ix = end_ix + n_val\n",
    "        # check if we are beyond the dataset\n",
    "        if (total_end_ix) > n_sequences:\n",
    "            print(f\"Training idx end at {end_ix}\")\n",
    "            print('Total idx checked', total_end_ix)\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = np.array(sequences.loc[:, sequences.columns != f\"{y_col}\"][i:end_ix]), np.array(\n",
    "            sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix-1])\n",
    "\n",
    "                                 \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    val_start_idx = start_idx + n_sequences - (start_idx  + n_val -2)\n",
    "    for i in range(val_start_idx, n_sequences):\n",
    "        if i == val_start_idx and input_verbose==1:\n",
    "            print(f\"Val idx start at {val_start_idx}\")\n",
    "        if (i % print_idx == 0) and i != 0 and input_verbose==1:\n",
    "            print(f\"Pct finished for val sequences = {i/n_sequences}\")\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences) and input_verbose==1:\n",
    "            print(f\"Val idx end at {end_ix}\")\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = np.array(sequences.loc[:, sequences.columns != f\"{y_col}\"][i:end_ix]), np.array(\n",
    "            sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix-1])\n",
    "        \n",
    "        \n",
    "        X_val.append(seq_x)\n",
    "        y_val.append(seq_y)\n",
    "    \n",
    "    \n",
    "\n",
    "    X, y, X_val, y_val = array(X), array(y), array(X_val), array(y_val)\n",
    "    \n",
    "    # errors for standard scaler\n",
    "    X = np.nan_to_num(X.astype(np.float32)) # converting to float 32 throws some infinity errors\n",
    "    X_val = np.nan_to_num(X_val.astype(np.float32)) # converting to float 32 throws some infinity errors  \n",
    "    \n",
    "    \n",
    "    scalers = {}\n",
    "    for i in range(X.shape[1]):\n",
    "        scalers[i] = StandardScaler()\n",
    "        X[:, i, :] = scalers[i].fit_transform(X[:, i, :]) \n",
    "    \n",
    "    pca_scalers = {}\n",
    "    N_COMPONENTS=100\n",
    "\n",
    "    new_X = np.zeros((X.shape[0], X.shape[1], N_COMPONENTS))\n",
    "    for i in range(X.shape[1]):\n",
    "        pca_scalers[i] = PCA(n_components=N_COMPONENTS) # ~80%\n",
    "        new_X[:, i, :] = pca_scalers[i].fit_transform(X[:, i, :]) \n",
    "\n",
    "\n",
    "    for i in range(X_val.shape[1]):\n",
    "        X_val[:, i, :] = scalers[i].transform(X_val[:, i, :]) \n",
    "\n",
    "\n",
    "    new_X_val = np.zeros((X_val.shape[0], X_val.shape[1], N_COMPONENTS))\n",
    "    for i in range(X_val.shape[1]):\n",
    "        new_X_val[:, i, :] = pca_scalers[i].transform(X_val[:, i, :]) \n",
    "        \n",
    "   # need  to do this again as standard scaler may have nans\n",
    "    X = np.nan_to_num(X.astype(np.float32)) # converting to float 32 throws some infinity errors\n",
    "    X_val = np.nan_to_num(X_val.astype(np.float32)) # converting to float 32 throws some infinity errors \n",
    "    print('X val shape', X_val.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    return new_X, y, new_X_val, y_val, scalers, pca_scalers\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sequences 597\n",
      "Training idx start at 0\n",
      "Pct finished = 0.16750418760469013\n",
      "Pct finished = 0.33500837520938026\n",
      "Pct finished = 0.5025125628140703\n",
      "Pct finished = 0.6700167504187605\n",
      "Pct finished = 0.8375209380234506\n",
      "Training idx end at 558\n",
      "Total idx checked 598\n",
      "Val idx start at 559\n",
      "Val idx end at 598\n",
      "X val shape (9, 30, 4956)\n"
     ]
    }
   ],
   "source": [
    "X,y, X_val, y_val, scalers, pca_scalers = split_sequences(\n",
    "    training_data,\n",
    "    30, start_idx=0, input_verbose=1,\n",
    "    n_val=40, y_col=f\"wal_mtb_close_corr_rolling_7_days\"\n",
    ") # 30 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(n_steps, n_features, n_units=100, dropout_pct=0.05, n_layers = 1):\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    # define CNN model\n",
    "#     model.add(TimeDistributed(Conv2D(n_units, kernel_and_pool_size))\n",
    "#     model.add(TimeDistributed(MaxPooling2D(pool_size=kernel_and_pool_size))\n",
    "#     model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        \n",
    "    model.add(LSTM(n_units, activation='relu', dropout=dropout_pct, return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    for _ in range(n_layers):\n",
    "        model.add(LSTM(n_units, activation='relu', dropout=dropout_pct, return_sequences=True))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_units, activation='relu', dropout=dropout_pct))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(n_units))\n",
    "    model.add(Dense(int(n_units/2)))\n",
    "    model.add(Dense(1))\n",
    "    #Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    #LR = 0.0001\n",
    "    #clipnorm=1., clipvalue=0.5\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False), loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_keras_model(30, 7610)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_264 (LSTM)              (None, 30, 100)           3084400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_255 (Bat (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_265 (LSTM)              (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_256 (Bat (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_266 (LSTM)              (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_257 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_261 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 3,261,601\n",
      "Trainable params: 3,261,001\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PredictionCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.validation_data[0])\n",
    "        print('prediction: {} at epoch: {}'.format(y_pred, epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on all data\n",
    "# predict for the upcoming week\n",
    "\n",
    "def prediction_for_upcoming_week(final_stock_df,pairs_of_stocks,  job_id=None, print_idx=1, n_day_sequences=14, \n",
    "                                 start_date_training_data='2018-01-01', n_validation_sequences=40, input_batch_size=128, \n",
    "                                 input_verbose=1):\n",
    "    \"\"\"\n",
    "    The main entrypoint for training an LSTM network on stock predictions\n",
    "    \n",
    "    :param final_stock_df: The list of stock pairs with correlations over different time ranges, volume\n",
    "    :para pairs_of_stocks: The list of stock pairs\n",
    "    :param print_idx: The number of iterations to pass before printing out progress\n",
    "    :param n_day_sequences: The number of sequences to pass to the LSTM (i.e. the number of days)\n",
    "    :param start_date_training_data: Filter for data before thie date to train on\n",
    "    :param n_validation_sequences: Number of sequences to validate on. Should be >= 40\n",
    "    :param input_batch-size\n",
    "    \"\"\"\n",
    "    # validation needs to be 40 or index error\n",
    "    final_stock_df = final_stock_df.dropna()\n",
    "    final_stock_df = final_stock_df.sort_values(by='date')\n",
    "    # add this to predictions\n",
    "    stock_to_industry = pd.read_csv('../data/Industries stock list - all.csv')\n",
    "    stock_to_industry.symbol = [i.lower() for i in stock_to_industry.symbol]\n",
    "\n",
    "    final_stock_df = final_stock_df.dropna()\n",
    "    most_recent_date = final_stock_df.index.max()\n",
    "\n",
    "    prediction_end = most_recent_date + datetime.timedelta(7)\n",
    "\n",
    "\n",
    "\n",
    "    test_df = final_stock_df.iloc[-n_day_sequences:, :]\n",
    "\n",
    "    \n",
    "\n",
    "    n_days_corr_predictions = 7\n",
    "\n",
    "\n",
    "    pct_change_corr = []\n",
    "    predicted_corr = []\n",
    "    last_corr_for_prediction_day = []\n",
    "    pred_dates = []\n",
    "    first_stock_industries = []\n",
    "    second_stock_industries = []\n",
    "    \n",
    "    first_model = True\n",
    "\n",
    "    start = time.time()\n",
    "    total_n = len(pairs_of_stocks)\n",
    "    \n",
    "    for idx,stock_pairing in enumerate(pairs_of_stocks):\n",
    "        if idx % print_idx == 0 and input_verbose ==1 :\n",
    "            print('----------')\n",
    "            print(f\"Stock pairing = {stock_pairing}\")\n",
    "            print(f\"Pct finished = {idx/total_n}\")\n",
    "        first_stock_name, second_stock_name = stock_pairing.split('_')\n",
    "\n",
    "        first_stock_industries.append(stock_to_industry[stock_to_industry.symbol == first_stock_name].industry.values[0])\n",
    "        second_stock_industries.append(stock_to_industry[stock_to_industry.symbol == second_stock_name].industry.values[0])\n",
    "\n",
    "\n",
    "\n",
    "        pred_col_name = f\"{stock_pairing}_close_corr_rolling_{n_days_corr_predictions}_days\"\n",
    "\n",
    "        # remove the current 7-day corr for this stock\n",
    "        # for 7 take rolling 7 days corr to the present day to predict off of\n",
    "        \n",
    "        ## TRAINING AND TESTING DATA\n",
    "        return final_stock_df[final_stock_df.index >= f\"{start_date_training_data}\"]\n",
    "        X,y, X_val, y_val, scalers, pca_scalers = split_sequences(\n",
    "            final_stock_df[final_stock_df.index >= f\"{start_date_training_data}\"],\n",
    "            n_day_sequences, start_idx=0, input_verbose=input_verbose,\n",
    "            n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
    "        ) # 30 steps\n",
    "        return X,y, X_val, y_val, scalers, pca_scalers \n",
    "\n",
    "        \n",
    "\n",
    "        train_X, train_y = final_stock_df.loc[:, final_stock_df.columns != f\"{pred_col_name}\"],  final_stock_df[f\"{pred_col_name}\"].shift(-7).fillna(method='ffill') \n",
    "                                                           # get corr from 7 days in the future\n",
    "        test_X, test_y = np.array(test_df.loc[:, test_df.columns != f\"{pred_col_name}\"]),  test_df[f\"{pred_col_name}\"]\n",
    "        test_X = test_X.reshape(1, test_X.shape[0], test_X.shape[1])\n",
    "        test_X = np.nan_to_num(test_X.astype(np.float32))\n",
    "        \n",
    "        for i in range(test_X.shape[1]):\n",
    "            test_X[:, i, :] = scalers[i].transform(test_X[:, i, :]) \n",
    "        test_X = np.nan_to_num(test_X.astype(np.float32))\n",
    "        \n",
    "        new_X_test = np.zeros((test_X.shape[0], test_X.shape[1], N_COMPONENTS))\n",
    "        for i in range(test_X.shape[1]):\n",
    "            new_X_test[:, i, :] = pca_scalers[i].transform(test_X[:, i, :]) \n",
    "        \n",
    "\n",
    "        \n",
    "#         return X,y, X_val, y_val, new_X_test\n",
    "        ## END TRAINING AND TESTING DATA \n",
    "        \n",
    "        \n",
    "        if first_model:\n",
    "            smaller_model = build_keras_model(X.shape[1],X.shape[2])\n",
    "            print(smaller_model.summary())\n",
    "            \n",
    "        # test again at 700 epochs\n",
    "        if first_model:\n",
    "            start = time.time()\n",
    "            # 800 epochs\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=200, verbose=1, restore_best_weights=True)\n",
    "            history = smaller_model.fit(x=X, y=y, batch_size=input_batch_size, epochs=200, verbose=input_verbose, \n",
    "                      validation_data=(X_val, y_val), shuffle=False,  use_multiprocessing=False, callbacks=[early_stopping])\n",
    "            end=time.time()\n",
    "\n",
    "            print((end-start)/60,' minutes')\n",
    "        else:\n",
    "\n",
    "            # Freeze the layers except the last 5 layers\n",
    "            for layer in smaller_model.layers[:-3]:\n",
    "                layer.trainable = False\n",
    "            # Check the trainable status of the individual layers\n",
    "\n",
    "#             for layer in smaller_model.layers:\n",
    "#                 print(layer, layer.trainable)\n",
    "\n",
    "#             smaller_model.compile(optimizer=Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False), loss='mse', metrics=['mse'])\n",
    "            \n",
    "            start = time.time()\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=1, restore_best_weights=True)\n",
    "            smaller_model = build_keras_model(X.shape[1],X.shape[2])\n",
    "            print(smaller_model.summary())\n",
    "            history = smaller_model.fit(x=X, y=y, batch_size=input_batch_size, epochs=200, verbose=input_verbose, \n",
    "                      validation_data=(X_val, y_val), shuffle=False,  use_multiprocessing=False, callbacks=[early_stopping])\n",
    "            end=time.time()\n",
    "            print((end-start)/60,' minutes')\n",
    "\n",
    "    \n",
    "        history_df  = pd.DataFrame(history.history)\n",
    "        history_df[['mse', 'val_mse']].iloc[-100:, :].plot()\n",
    "        plt.show()\n",
    "        prediction = smaller_model.predict(new_X_test)[0][0] \n",
    "\n",
    "        if idx % print_idx==0:\n",
    "            print(f\"Prediction = {prediction}\")\n",
    "\n",
    "\n",
    "\n",
    "        last_corr_date = train_y.index.max()\n",
    "        last_corr = train_y[train_y.index.max()]  \n",
    "        if idx % print_idx==0:\n",
    "            print(f\"Last corr = {last_corr}\")\n",
    "\n",
    "        pred_dates.append(most_recent_date)\n",
    "        predicted_corr.append(prediction)\n",
    "        last_corr_for_prediction_day.append(last_corr)\n",
    "        \n",
    "        if input_verbose==1 and idx % print_idx==0:\n",
    "            print(f\"{stock_pairing} corr7-day corr of close from {most_recent_date} to {prediction_end} is {prediction} \")\n",
    "        \n",
    "        first_model = False\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Predictions took {(end-start)/60} mins\")\n",
    "\n",
    "    squarred_difference = (np.array(last_corr_for_prediction_day)-np.array(predicted_corr))**2\n",
    "\n",
    "    prediction_df = pd.DataFrame({ 'pred_date_start':pred_dates,'stock_pair':pairs_of_stocks,   'first_stock_industry': first_stock_industries, \n",
    "                   'second_stock_industry': second_stock_industries,\n",
    "                   'predicted_corr': predicted_corr, 'last_7_day_corr_for_pred_date_start': last_corr_for_prediction_day, \n",
    "            'squarred_diff_7_day_cor': (np.array(last_corr_for_prediction_day)-np.array(predicted_corr))**2\n",
    "                 })\n",
    "    \n",
    "    if job_id:\n",
    "        tmp_filepath = '../data/lstm_tmp_prediction_dfs'\n",
    "        if not os.path.isdir(f\"{tmp_filepath}\"):\n",
    "            os.mkdir(f\"{tmp_filepath}\")\n",
    "        prediction_df.to_csv(\n",
    "        f'{tmp_filepath}/{job_id}_lstm_test_predictions_{most_recent_date}-{prediction_end}.csv', index=False)\n",
    "    else:\n",
    "        prediction_df.to_csv(\n",
    "    f'../data/predictions/lstm_test_predictions_{most_recent_date}-{prediction_end}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741"
      ]
     },
     "execution_count": 1130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_of_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]\n",
    "\n",
    "def do_job(job_id, stock_pairs, data_slice):\n",
    "    all_prediction_df = []\n",
    "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
    "\n",
    "\n",
    "\n",
    "def dispatch_jobs(data, job_number, pairs):\n",
    "    total = len(pairs)\n",
    "    chunk_size = total / job_number\n",
    "    slice = chunks(pairs, int(chunk_size))\n",
    "    jobs = []\n",
    "\n",
    "    for i, pair in enumerate(slice):\n",
    "        j = Process(target=do_job, args=(i, pair, data))\n",
    "        jobs.append(j)\n",
    "    for j in jobs:\n",
    "        j.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2523 len input final_stock_df\n",
      "2523 len input final_stock_df\n",
      "2523 len input final_stock_df\n",
      "2523 len input final_stock_df\n",
      "2523 len input final_stock_df\n",
      "2523 len input final_stock_df\n",
      "2523 len input final_stock_df\n",
      "2523 len input final_stock_df\n",
      "2523 len input final_stock_df\n",
      "n_sequences 597\n",
      "n_sequences 597\n",
      "n_sequences 597\n",
      "n_sequences 597\n",
      "n_sequences 597\n",
      "n_sequences 597\n",
      "n_sequences 597\n",
      "n_sequences 597\n",
      "n_sequences 597\n",
      "Training idx end at 568\n",
      "Total idx checked 598\n",
      "Training idx end at 568\n",
      "Total idx checked 598\n",
      "Training idx end at 568\n",
      "Total idx checked 598\n",
      "Training idx end at 568\n",
      "Total idx checked 598\n",
      "Training idx end at 568\n",
      "Total idx checked 598\n",
      "Training idx end at 568\n",
      "Total idx checked 598\n",
      "Training idx end at 568\n",
      "Total idx checked 598\n",
      "Training idx end at 568\n",
      "Total idx checked 598\n",
      "Training idx end at 568\n",
      "Total idx checked 598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-21:\n",
      "Process Process-19:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-1112-bbbb3f9032f4>\", line 6, in do_job\n",
      "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
      "  File \"<ipython-input-1110-093a2f63fce5>\", line 61, in prediction_for_upcoming_week\n",
      "    n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
      "  File \"<ipython-input-1105-c88a42d08761>\", line 54, in split_sequences\n",
      "    sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix])\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1767, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2137, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2062, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "  File \"<ipython-input-1112-bbbb3f9032f4>\", line 6, in do_job\n",
      "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
      "IndexError: single positional indexer is out-of-bounds\n",
      "  File \"<ipython-input-1110-093a2f63fce5>\", line 61, in prediction_for_upcoming_week\n",
      "    n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
      "  File \"<ipython-input-1105-c88a42d08761>\", line 54, in split_sequences\n",
      "    sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix])\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1767, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2137, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2062, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "IndexError: single positional indexer is out-of-bounds\n",
      "Process Process-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-1112-bbbb3f9032f4>\", line 6, in do_job\n",
      "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
      "  File \"<ipython-input-1110-093a2f63fce5>\", line 61, in prediction_for_upcoming_week\n",
      "    n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
      "  File \"<ipython-input-1105-c88a42d08761>\", line 54, in split_sequences\n",
      "    sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix])\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1767, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2137, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2062, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "IndexError: single positional indexer is out-of-bounds\n",
      "Process Process-22:\n",
      "Process Process-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-1112-bbbb3f9032f4>\", line 6, in do_job\n",
      "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
      "  File \"<ipython-input-1110-093a2f63fce5>\", line 61, in prediction_for_upcoming_week\n",
      "    n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
      "  File \"<ipython-input-1112-bbbb3f9032f4>\", line 6, in do_job\n",
      "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
      "  File \"<ipython-input-1105-c88a42d08761>\", line 54, in split_sequences\n",
      "    sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix])\n",
      "  File \"<ipython-input-1110-093a2f63fce5>\", line 61, in prediction_for_upcoming_week\n",
      "    n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1767, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"<ipython-input-1105-c88a42d08761>\", line 54, in split_sequences\n",
      "    sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix])\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2137, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1767, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2137, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2062, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2062, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "IndexError: single positional indexer is out-of-bounds\n",
      "IndexError: single positional indexer is out-of-bounds\n",
      "Process Process-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-1112-bbbb3f9032f4>\", line 6, in do_job\n",
      "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
      "  File \"<ipython-input-1110-093a2f63fce5>\", line 61, in prediction_for_upcoming_week\n",
      "    n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
      "  File \"<ipython-input-1105-c88a42d08761>\", line 54, in split_sequences\n",
      "    sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix])\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1767, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2137, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2062, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "IndexError: single positional indexer is out-of-bounds\n",
      "Process Process-24:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-1112-bbbb3f9032f4>\", line 6, in do_job\n",
      "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
      "  File \"<ipython-input-1110-093a2f63fce5>\", line 61, in prediction_for_upcoming_week\n",
      "    n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
      "Process Process-25:\n",
      "  File \"<ipython-input-1105-c88a42d08761>\", line 54, in split_sequences\n",
      "    sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix])\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1767, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2137, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2062, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "IndexError: single positional indexer is out-of-bounds\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-1112-bbbb3f9032f4>\", line 6, in do_job\n",
      "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
      "  File \"<ipython-input-1110-093a2f63fce5>\", line 61, in prediction_for_upcoming_week\n",
      "    n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
      "  File \"<ipython-input-1105-c88a42d08761>\", line 54, in split_sequences\n",
      "    sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix])\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1767, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2137, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2062, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "IndexError: single positional indexer is out-of-bounds\n",
      "Process Process-27:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jonathanhilgart/.pyenv/versions/3.7.0/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-1112-bbbb3f9032f4>\", line 6, in do_job\n",
      "    prediction_for_upcoming_week(data_slice, stock_pairs, job_id=job_id)\n",
      "  File \"<ipython-input-1110-093a2f63fce5>\", line 61, in prediction_for_upcoming_week\n",
      "    n_val=n_validation_sequences, y_col=f\"{pred_col_name}\"\n",
      "  File \"<ipython-input-1105-c88a42d08761>\", line 54, in split_sequences\n",
      "    sequences.loc[:, sequences.columns == f\"{y_col}\"].shift(-7).fillna(method='ffill').iloc[end_ix])\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1767, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2137, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"/Users/jonathanhilgart/Library/Caches/pypoetry/virtualenvs/daily-trading-nJ43NNNI-py3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2062, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "IndexError: single positional indexer is out-of-bounds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    final_stock_df = final_stock_df.dropna()\n",
    "    num_workers = mp.cpu_count()  \n",
    "    dispatch_jobs(final_stock_df, num_workers , pairs_of_stocks[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2523 entries, 2005-06-30 to 2020-05-15\n",
      "Columns: 4957 entries, wal_mtb_close_corr_rolling_5_days to year\n",
      "dtypes: float64(4953), int64(4)\n",
      "memory usage: 95.4 MB\n"
     ]
    }
   ],
   "source": [
    "final_stock_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Stock pairing = wal_mtb\n",
      "Pct finished = 0.0\n"
     ]
    }
   ],
   "source": [
    "# 6.5 minutes for 10 stocks\n",
    "final_stock_df = final_stock_df.dropna()\n",
    "# test 14 day period instead of 30\n",
    "# no dice\n",
    "# test smaller network smae LR\n",
    "# no dice\n",
    "# test since 2016 data\n",
    "# no dice\n",
    "# test smaller learning rate \n",
    "# no dice\n",
    "# test smaller batch size\n",
    "# nothing\n",
    "# test batch size  \n",
    "\n",
    "\n",
    "# so the solution was less validation data\n",
    "\n",
    "# wal_cwbc is turns out needed less training data\n",
    "\n",
    "# test a new model for each pair, 200 epochs per . 2 minutes per 200 epochs\n",
    "# 300 pairs take 10 hours sequentially\n",
    "# X,y, X_val, y_val, scalers, pca_scalers \n",
    "training_data= prediction_for_upcoming_week(final_stock_df, pairs_of_stocks[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wal_mtb_close_corr_rolling_5_days</th>\n",
       "      <th>wal_close_std_rolling_5_days</th>\n",
       "      <th>mtb_close_std_rolling_5_days</th>\n",
       "      <th>wal_volume_std_rolling_5_days</th>\n",
       "      <th>mtb_volume_std_rolling_5_days</th>\n",
       "      <th>wal_volume</th>\n",
       "      <th>mtb_volume</th>\n",
       "      <th>wal_mtb_close_corr_rolling_7_days</th>\n",
       "      <th>wal_close_std_rolling_7_days</th>\n",
       "      <th>mtb_close_std_rolling_7_days</th>\n",
       "      <th>...</th>\n",
       "      <th>cpt_reg_close_corr_rolling_5_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_7_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_10_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_30_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_180_days</th>\n",
       "      <th>cpt_reg_close_corr_rolling_365_days</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>0.969491</td>\n",
       "      <td>0.242590</td>\n",
       "      <td>0.540629</td>\n",
       "      <td>71459.939653</td>\n",
       "      <td>82750.470242</td>\n",
       "      <td>389396.0</td>\n",
       "      <td>424392.0</td>\n",
       "      <td>0.982199</td>\n",
       "      <td>0.644249</td>\n",
       "      <td>0.961794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522325</td>\n",
       "      <td>0.887355</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.098938</td>\n",
       "      <td>0.464441</td>\n",
       "      <td>-0.388416</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>0.666366</td>\n",
       "      <td>0.223271</td>\n",
       "      <td>0.870747</td>\n",
       "      <td>76557.999255</td>\n",
       "      <td>87630.342220</td>\n",
       "      <td>338118.0</td>\n",
       "      <td>437559.0</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.438623</td>\n",
       "      <td>0.848135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091648</td>\n",
       "      <td>0.765359</td>\n",
       "      <td>0.908782</td>\n",
       "      <td>0.126810</td>\n",
       "      <td>0.486648</td>\n",
       "      <td>-0.386528</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>0.721908</td>\n",
       "      <td>0.240520</td>\n",
       "      <td>1.102034</td>\n",
       "      <td>158775.994394</td>\n",
       "      <td>143127.496064</td>\n",
       "      <td>611753.0</td>\n",
       "      <td>628314.0</td>\n",
       "      <td>0.662799</td>\n",
       "      <td>0.207961</td>\n",
       "      <td>0.931213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962028</td>\n",
       "      <td>0.961260</td>\n",
       "      <td>0.953959</td>\n",
       "      <td>0.322918</td>\n",
       "      <td>0.508217</td>\n",
       "      <td>-0.386158</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.578692</td>\n",
       "      <td>0.463271</td>\n",
       "      <td>1.137299</td>\n",
       "      <td>128230.249714</td>\n",
       "      <td>94217.107971</td>\n",
       "      <td>425203.0</td>\n",
       "      <td>549691.0</td>\n",
       "      <td>0.590420</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>0.969475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975050</td>\n",
       "      <td>0.977253</td>\n",
       "      <td>0.965353</td>\n",
       "      <td>0.471573</td>\n",
       "      <td>0.530037</td>\n",
       "      <td>-0.384616</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.280348</td>\n",
       "      <td>0.400849</td>\n",
       "      <td>0.870994</td>\n",
       "      <td>111338.714266</td>\n",
       "      <td>84419.407121</td>\n",
       "      <td>348035.0</td>\n",
       "      <td>488878.0</td>\n",
       "      <td>0.551488</td>\n",
       "      <td>0.407472</td>\n",
       "      <td>0.929618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981058</td>\n",
       "      <td>0.978178</td>\n",
       "      <td>0.967308</td>\n",
       "      <td>0.543734</td>\n",
       "      <td>0.548609</td>\n",
       "      <td>-0.381319</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>0.766228</td>\n",
       "      <td>0.868637</td>\n",
       "      <td>2.179638</td>\n",
       "      <td>52817.185241</td>\n",
       "      <td>149191.991991</td>\n",
       "      <td>757758.0</td>\n",
       "      <td>897767.0</td>\n",
       "      <td>0.748908</td>\n",
       "      <td>0.777775</td>\n",
       "      <td>2.790674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820796</td>\n",
       "      <td>0.621151</td>\n",
       "      <td>0.773188</td>\n",
       "      <td>0.790651</td>\n",
       "      <td>0.947226</td>\n",
       "      <td>0.762557</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-12</th>\n",
       "      <td>0.959172</td>\n",
       "      <td>1.505380</td>\n",
       "      <td>3.481016</td>\n",
       "      <td>93169.966213</td>\n",
       "      <td>232839.250455</td>\n",
       "      <td>842800.0</td>\n",
       "      <td>1054813.0</td>\n",
       "      <td>0.921566</td>\n",
       "      <td>1.291135</td>\n",
       "      <td>3.298373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>0.833468</td>\n",
       "      <td>0.809434</td>\n",
       "      <td>0.793117</td>\n",
       "      <td>0.947982</td>\n",
       "      <td>0.765509</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-13</th>\n",
       "      <td>0.982959</td>\n",
       "      <td>2.297035</td>\n",
       "      <td>5.954967</td>\n",
       "      <td>252559.966011</td>\n",
       "      <td>235833.161623</td>\n",
       "      <td>1243691.0</td>\n",
       "      <td>970433.0</td>\n",
       "      <td>0.965713</td>\n",
       "      <td>1.897660</td>\n",
       "      <td>5.166668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891453</td>\n",
       "      <td>0.860975</td>\n",
       "      <td>0.796259</td>\n",
       "      <td>0.795974</td>\n",
       "      <td>0.948399</td>\n",
       "      <td>0.768252</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14</th>\n",
       "      <td>0.986109</td>\n",
       "      <td>2.456406</td>\n",
       "      <td>6.032050</td>\n",
       "      <td>240029.632601</td>\n",
       "      <td>243475.803784</td>\n",
       "      <td>1044445.0</td>\n",
       "      <td>1155514.0</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>2.123624</td>\n",
       "      <td>5.428586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954624</td>\n",
       "      <td>0.906905</td>\n",
       "      <td>0.810136</td>\n",
       "      <td>0.799692</td>\n",
       "      <td>0.948965</td>\n",
       "      <td>0.771334</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-15</th>\n",
       "      <td>0.932608</td>\n",
       "      <td>1.654282</td>\n",
       "      <td>4.280231</td>\n",
       "      <td>225816.719524</td>\n",
       "      <td>119314.840351</td>\n",
       "      <td>693076.0</td>\n",
       "      <td>861633.0</td>\n",
       "      <td>0.974008</td>\n",
       "      <td>2.243275</td>\n",
       "      <td>6.121549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911496</td>\n",
       "      <td>0.927717</td>\n",
       "      <td>0.892990</td>\n",
       "      <td>0.791171</td>\n",
       "      <td>0.949431</td>\n",
       "      <td>0.774133</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>597 rows  4957 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            wal_mtb_close_corr_rolling_5_days  wal_close_std_rolling_5_days  \\\n",
       "date                                                                          \n",
       "2018-01-02                           0.969491                      0.242590   \n",
       "2018-01-03                           0.666366                      0.223271   \n",
       "2018-01-04                           0.721908                      0.240520   \n",
       "2018-01-05                           0.578692                      0.463271   \n",
       "2018-01-08                           0.280348                      0.400849   \n",
       "...                                       ...                           ...   \n",
       "2020-05-11                           0.766228                      0.868637   \n",
       "2020-05-12                           0.959172                      1.505380   \n",
       "2020-05-13                           0.982959                      2.297035   \n",
       "2020-05-14                           0.986109                      2.456406   \n",
       "2020-05-15                           0.932608                      1.654282   \n",
       "\n",
       "            mtb_close_std_rolling_5_days  wal_volume_std_rolling_5_days  \\\n",
       "date                                                                      \n",
       "2018-01-02                      0.540629                   71459.939653   \n",
       "2018-01-03                      0.870747                   76557.999255   \n",
       "2018-01-04                      1.102034                  158775.994394   \n",
       "2018-01-05                      1.137299                  128230.249714   \n",
       "2018-01-08                      0.870994                  111338.714266   \n",
       "...                                  ...                            ...   \n",
       "2020-05-11                      2.179638                   52817.185241   \n",
       "2020-05-12                      3.481016                   93169.966213   \n",
       "2020-05-13                      5.954967                  252559.966011   \n",
       "2020-05-14                      6.032050                  240029.632601   \n",
       "2020-05-15                      4.280231                  225816.719524   \n",
       "\n",
       "            mtb_volume_std_rolling_5_days  wal_volume  mtb_volume  \\\n",
       "date                                                                \n",
       "2018-01-02                   82750.470242    389396.0    424392.0   \n",
       "2018-01-03                   87630.342220    338118.0    437559.0   \n",
       "2018-01-04                  143127.496064    611753.0    628314.0   \n",
       "2018-01-05                   94217.107971    425203.0    549691.0   \n",
       "2018-01-08                   84419.407121    348035.0    488878.0   \n",
       "...                                   ...         ...         ...   \n",
       "2020-05-11                  149191.991991    757758.0    897767.0   \n",
       "2020-05-12                  232839.250455    842800.0   1054813.0   \n",
       "2020-05-13                  235833.161623   1243691.0    970433.0   \n",
       "2020-05-14                  243475.803784   1044445.0   1155514.0   \n",
       "2020-05-15                  119314.840351    693076.0    861633.0   \n",
       "\n",
       "            wal_mtb_close_corr_rolling_7_days  wal_close_std_rolling_7_days  \\\n",
       "date                                                                          \n",
       "2018-01-02                           0.982199                      0.644249   \n",
       "2018-01-03                           0.726948                      0.438623   \n",
       "2018-01-04                           0.662799                      0.207961   \n",
       "2018-01-05                           0.590420                      0.388716   \n",
       "2018-01-08                           0.551488                      0.407472   \n",
       "...                                       ...                           ...   \n",
       "2020-05-11                           0.748908                      0.777775   \n",
       "2020-05-12                           0.921566                      1.291135   \n",
       "2020-05-13                           0.965713                      1.897660   \n",
       "2020-05-14                           0.982818                      2.123624   \n",
       "2020-05-15                           0.974008                      2.243275   \n",
       "\n",
       "            mtb_close_std_rolling_7_days  ...  \\\n",
       "date                                      ...   \n",
       "2018-01-02                      0.961794  ...   \n",
       "2018-01-03                      0.848135  ...   \n",
       "2018-01-04                      0.931213  ...   \n",
       "2018-01-05                      0.969475  ...   \n",
       "2018-01-08                      0.929618  ...   \n",
       "...                                  ...  ...   \n",
       "2020-05-11                      2.790674  ...   \n",
       "2020-05-12                      3.298373  ...   \n",
       "2020-05-13                      5.166668  ...   \n",
       "2020-05-14                      5.428586  ...   \n",
       "2020-05-15                      6.121549  ...   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_5_days  \\\n",
       "date                                            \n",
       "2018-01-02                           0.522325   \n",
       "2018-01-03                          -0.091648   \n",
       "2018-01-04                           0.962028   \n",
       "2018-01-05                           0.975050   \n",
       "2018-01-08                           0.981058   \n",
       "...                                       ...   \n",
       "2020-05-11                           0.820796   \n",
       "2020-05-12                           0.881267   \n",
       "2020-05-13                           0.891453   \n",
       "2020-05-14                           0.954624   \n",
       "2020-05-15                           0.911496   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_7_days  \\\n",
       "date                                            \n",
       "2018-01-02                           0.887355   \n",
       "2018-01-03                           0.765359   \n",
       "2018-01-04                           0.961260   \n",
       "2018-01-05                           0.977253   \n",
       "2018-01-08                           0.978178   \n",
       "...                                       ...   \n",
       "2020-05-11                           0.621151   \n",
       "2020-05-12                           0.833468   \n",
       "2020-05-13                           0.860975   \n",
       "2020-05-14                           0.906905   \n",
       "2020-05-15                           0.927717   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_10_days  \\\n",
       "date                                             \n",
       "2018-01-02                            0.928270   \n",
       "2018-01-03                            0.908782   \n",
       "2018-01-04                            0.953959   \n",
       "2018-01-05                            0.965353   \n",
       "2018-01-08                            0.967308   \n",
       "...                                        ...   \n",
       "2020-05-11                            0.773188   \n",
       "2020-05-12                            0.809434   \n",
       "2020-05-13                            0.796259   \n",
       "2020-05-14                            0.810136   \n",
       "2020-05-15                            0.892990   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_30_days  \\\n",
       "date                                             \n",
       "2018-01-02                            0.098938   \n",
       "2018-01-03                            0.126810   \n",
       "2018-01-04                            0.322918   \n",
       "2018-01-05                            0.471573   \n",
       "2018-01-08                            0.543734   \n",
       "...                                        ...   \n",
       "2020-05-11                            0.790651   \n",
       "2020-05-12                            0.793117   \n",
       "2020-05-13                            0.795974   \n",
       "2020-05-14                            0.799692   \n",
       "2020-05-15                            0.791171   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_180_days  \\\n",
       "date                                              \n",
       "2018-01-02                             0.464441   \n",
       "2018-01-03                             0.486648   \n",
       "2018-01-04                             0.508217   \n",
       "2018-01-05                             0.530037   \n",
       "2018-01-08                             0.548609   \n",
       "...                                         ...   \n",
       "2020-05-11                             0.947226   \n",
       "2020-05-12                             0.947982   \n",
       "2020-05-13                             0.948399   \n",
       "2020-05-14                             0.948965   \n",
       "2020-05-15                             0.949431   \n",
       "\n",
       "            cpt_reg_close_corr_rolling_365_days  day  month  quarter  year  \n",
       "date                                                                        \n",
       "2018-01-02                            -0.388416    2      1        1  2018  \n",
       "2018-01-03                            -0.386528    3      1        1  2018  \n",
       "2018-01-04                            -0.386158    4      1        1  2018  \n",
       "2018-01-05                            -0.384616    5      1        1  2018  \n",
       "2018-01-08                            -0.381319    8      1        1  2018  \n",
       "...                                         ...  ...    ...      ...   ...  \n",
       "2020-05-11                             0.762557   11      5        2  2020  \n",
       "2020-05-12                             0.765509   12      5        2  2020  \n",
       "2020-05-13                             0.768252   13      5        2  2020  \n",
       "2020-05-14                             0.771334   14      5        2  2020  \n",
       "2020-05-15                             0.774133   15      5        2  2020  \n",
       "\n",
       "[597 rows x 4957 columns]"
      ]
     },
     "execution_count": 1135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(X_val).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(new_X_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('daily-trading-nJ43NNNI-py3.7': venv)",
   "language": "python",
   "name": "python37064bitdailytradingnj43nnnipy37venvd9db9359ec934f90b15f4732101b653e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
